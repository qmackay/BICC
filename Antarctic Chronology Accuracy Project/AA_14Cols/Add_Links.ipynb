{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import os\n",
    "import yaml\n",
    "import itertools\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DF</th>\n",
       "      <th>EDC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11.26</td>\n",
       "      <td>12.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11.68</td>\n",
       "      <td>12.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18.90</td>\n",
       "      <td>19.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20.38</td>\n",
       "      <td>21.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21.41</td>\n",
       "      <td>22.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1396</th>\n",
       "      <td>2178.26</td>\n",
       "      <td>2161.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1397</th>\n",
       "      <td>2179.03</td>\n",
       "      <td>2163.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1398</th>\n",
       "      <td>2179.33</td>\n",
       "      <td>2163.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1399</th>\n",
       "      <td>2180.00</td>\n",
       "      <td>2164.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1400</th>\n",
       "      <td>2184.12</td>\n",
       "      <td>2170.34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1401 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           DF      EDC\n",
       "0       11.26    12.11\n",
       "1       11.68    12.55\n",
       "2       18.90    19.55\n",
       "3       20.38    21.09\n",
       "4       21.41    22.03\n",
       "...       ...      ...\n",
       "1396  2178.26  2161.98\n",
       "1397  2179.03  2163.18\n",
       "1398  2179.33  2163.64\n",
       "1399  2180.00  2164.54\n",
       "1400  2184.12  2170.34\n",
       "\n",
       "[1401 rows x 2 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sv_links = pd.read_excel('/Users/quinnmackay/Documents/GitHub/BICC/Data Storage/Tiepoints/DF-EDC_Fujita.xlsx', sheet_name=0, skiprows=11, usecols=[1,2], names=['DF', 'EDC']) \n",
    "\n",
    "comment_add = 'Includes DF-EDC99 links from Supplemental C of Fujita et al. 2015 (Clim. Past)'\n",
    "sv_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backups not enabled.\n"
     ]
    }
   ],
   "source": [
    "backups=False\n",
    "folder='AA_14Cols'\n",
    "error=10\n",
    "tolerance = 0.10  # meters\n",
    "\n",
    "if backups == True:\n",
    "    # add backup of BICC\n",
    "\n",
    "    source_folder = f'/Users/quinnmackay/Documents/GitHub/BICC/Antarctic Chronology Accuracy Project/{folder}'\n",
    "    backup_root = f'/Users/quinnmackay/Documents/GitHub/BICC/Antarctic Chronology Accuracy Project/{folder}/Backups'\n",
    "\n",
    "    # Create timestamp\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    backup_folder = os.path.join(backup_root, f'BICC_Backup_{timestamp}')\n",
    "\n",
    "    # Make sure the backup root exists\n",
    "    os.makedirs(backup_root, exist_ok=True)\n",
    "\n",
    "    # Copy the folder\n",
    "    shutil.copytree(source_folder, backup_folder)\n",
    "\n",
    "    print(f\"Backup completed: {backup_folder}\")\n",
    "else:\n",
    "    print(\"Backups not enabled.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load cores list from params\n",
    "params = f'/Users/quinnmackay/Documents/GitHub/BICC/Antarctic Chronology Accuracy Project/{folder}/parameters.yml'\n",
    "with open(params, 'r') as f:\n",
    "    first_line = f.readline()\n",
    "params_load = yaml.safe_load(first_line)\n",
    "list_sites = params_load['list_sites']\n",
    "\n",
    "# get all link combos\n",
    "pairs = [f\"{a}-{b}\" for a, b in itertools.combinations(list_sites, 2)]\n",
    "\n",
    "#get all combos possible from the svensson links\n",
    "subset = list(sv_links.columns)\n",
    "valid_pairs = [p for p in pairs if all(site in subset for site in p.split('-'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped EDC-DF — file not found\n"
     ]
    }
   ],
   "source": [
    "#now, create all the link files\n",
    "sv_synchros = {}\n",
    "sv_comments = {}\n",
    "for pair in valid_pairs:\n",
    "    site_a, site_b = pair.split('-')\n",
    "\n",
    "    # Drop NaN rows to get only valid shared tiepoints\n",
    "    sv_synchros[pair] = sv_links[[site_a, site_b]].dropna()\n",
    "    sv_synchros[pair].columns = ['depth1', 'depth2']\n",
    "    sv_synchros[pair]['age_unc'] = error\n",
    "    sv_synchros[pair]['comment'] = np.nan  # Initialize age column with NaN\n",
    "\n",
    "    sv_comments[pair] = [comment_add]\n",
    "\n",
    "#now load existing synchros\n",
    "os.chdir(f'/Users/quinnmackay/Documents/GitHub/BICC/Antarctic Chronology Accuracy Project/{folder}')\n",
    "existing_synchros = {}\n",
    "existing_comments = {}\n",
    "\n",
    "for pair in valid_pairs:\n",
    "    folder_path = pair\n",
    "    file_path = os.path.join(folder_path, \"iceice_synchro_horizons.txt\")\n",
    "\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "\n",
    "        comments = []\n",
    "        with open(file_path, 'r') as f: # store existing comments to be re-added\n",
    "            for line in f:\n",
    "                if line.startswith('#'):\n",
    "                    comments.append(line.rstrip('\\n'))  # keeps tabs intact  # store comment lines without newline\n",
    "        existing_comments[pair] = comments\n",
    "\n",
    "        try:\n",
    "            synchros = pd.read_csv(file_path, sep='\\t', comment='#')\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {pair}: {e}\")\n",
    "            print(f'File Path: {file_path}')\n",
    "            sys.exit()\n",
    "\n",
    "        existing_synchros[pair] = synchros\n",
    "        print(f\"Loaded {pair}: {len(synchros)} rows\")\n",
    "    else:\n",
    "        print(f\"Skipped {pair} — file not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No existing synchros for EDC-DF, saving new synchro to EDC-DF/iceice_synchro_horizons.txt\n"
     ]
    }
   ],
   "source": [
    "for pair in valid_pairs:\n",
    "\n",
    "    if pair in existing_synchros:\n",
    "        \n",
    "        overlaps = 0\n",
    "        new_ties = 0\n",
    "        error_overlaps = 0\n",
    "\n",
    "        existing = existing_synchros[pair].copy(deep=True)\n",
    "        new = sv_synchros[pair].copy(deep=True)\n",
    "\n",
    "        total_existing = len(existing)\n",
    "\n",
    "        # Start with originals (priority)\n",
    "        combined = existing.copy(deep=True)\n",
    "\n",
    "        # Append only existing tiepoints that are NOT close to any new tiepoint\n",
    "        for idx, row in new.iterrows():\n",
    "\n",
    "            mask_either = ( #check for either column within tolerance. Does NOT append if either is close.\n",
    "                (abs(existing.iloc[:, 0] - row.iloc[0]) <= tolerance) |\n",
    "                (abs(existing.iloc[:, 1] - row.iloc[1]) <= tolerance)\n",
    "            )\n",
    "            mask_both = ( #check for both columns to spit out error if needed\n",
    "                (abs(existing.iloc[:, 0] - row.iloc[0]) <= tolerance) &\n",
    "                (abs(existing.iloc[:, 1] - row.iloc[1]) <= tolerance)\n",
    "            )\n",
    "\n",
    "            if not mask_either.any():\n",
    "                combined = pd.concat([combined, row.to_frame().T], ignore_index=True)\n",
    "                new_ties+=1\n",
    "            else:\n",
    "                overlaps += 1\n",
    "\n",
    "            if mask_either.any() and not mask_both.any():\n",
    "                print(f\"Warning: Partial overlap detected for pair {pair} at new tiepoint {row.values}. One column is within {tolerance} of an existing tiepoint, while other is not. The tiepoint was not added to avoid conflicts.\")\n",
    "                error_overlaps += 1\n",
    "\n",
    "        #duplicate check\n",
    "        before = len(combined)\n",
    "        combined = combined.drop_duplicates(ignore_index=True)\n",
    "        after = len(combined)\n",
    "\n",
    "        if after < before:\n",
    "            print(f\"Warning: {before - after} duplicate tiepoint(s) found and removed in pair {pair}.\")\n",
    "\n",
    "        folder_path = pair\n",
    "        file_path = os.path.join(folder_path, \"iceice_synchro_horizons.txt\")\n",
    "        os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "        # --- Gather comments ---\n",
    "        new_comments = sv_comments.get(pair, [])\n",
    "\n",
    "        with open(file_path, 'w') as f:\n",
    "            # Write new comments first\n",
    "            if new_ties > 0:\n",
    "                for line in new_comments:\n",
    "                    f.write(f\"#{line}\\n\")\n",
    "            # Add original comments\n",
    "            for line in existing_comments.get(pair, []):\n",
    "                f.write(f\"#{line}\\n\")\n",
    "            # Write merged DataFrame below\n",
    "            combined.to_csv(f, sep='\\t', index=False)\n",
    "        print(f\"Saved merged synchro for {pair} with rows to {file_path}\\nThere were {total_existing} existing rows, {overlaps} overlaps ({error_overlaps} error overlaps), and {new_ties} new ties.\")\n",
    "\n",
    "    else:\n",
    "        folder_path = pair\n",
    "        file_path = os.path.join(folder_path, \"iceice_synchro_horizons.txt\")\n",
    "        os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "        combined = sv_synchros[pair].copy(deep=True) # svensson links load\n",
    "        new_comments = sv_comments.get(pair, [])\n",
    "\n",
    "        with open(file_path, 'w') as f:\n",
    "            # Write new comments first\n",
    "            for line in new_comments:\n",
    "                f.write(f\"#{line}\\n\")\n",
    "            # Write merged DataFrame below\n",
    "            combined.to_csv(f, sep='\\t', index=False)\n",
    "\n",
    "        print(f\"No existing synchros for {pair}, saving new synchro to {file_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bicc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
