{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import os\n",
    "import yaml\n",
    "import itertools\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from openpyxl.styles import PatternFill\n",
    "from openpyxl import load_workbook\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "from openpyxl.styles import Border, Side\n",
    "from openpyxl.styles import Alignment, Font\n",
    "from openpyxl.utils import get_column_letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cores list of sites for AA\n",
    "\n",
    "project = 'Greenland'\n",
    "output_dir = '/Users/quinnmackay/Desktop/table_out'\n",
    "\n",
    "# get all link combos\n",
    "with open(f'/Users/quinnmackay/Documents/GitHub/BICC/Antarctic Chronology Accuracy Project/{project}/parameters.yml') as f:\n",
    "    data = yaml.safe_load(f)\n",
    "list_sites = data[\"list_sites\"]\n",
    "pairs = [f\"{a}-{b}\" for a, b in itertools.combinations(list_sites, 2)]\n",
    "\n",
    "error_margin = 0.1\n",
    "big_error_margin = 0.25\n",
    "base_core_age = 'EDC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed pair GISP2-NG1, total points after merging: 290, (290 original total rows)\n",
      "Processed pair GISP2-NG2, total points after merging: 619, (725 original total rows)\n",
      "Processed pair GISP2-NEEM, total points after merging: 194, (194 original total rows)\n",
      "Processed pair GISP2-GRIP, total points after merging: 825, (918 original total rows)\n",
      "Processed pair NG1-NG2, total points after merging: 311, (311 original total rows)\n",
      "Processed pair NG1-NEEM, total points after merging: 578, (673 original total rows)\n",
      "Processed pair NG1-GRIP, total points after merging: 530, (587 original total rows)\n",
      "Processed pair NG2-NEEM, total points after merging: 824, (886 original total rows)\n",
      "Processed pair NG2-GRIP, total points after merging: 907, (1003 original total rows)\n",
      "Processed pair NEEM-GRIP, total points after merging: 396, (396 original total rows)\n"
     ]
    }
   ],
   "source": [
    "big_table = pd.DataFrame()\n",
    "\n",
    "for core in list_sites: # loop through each core\n",
    "    for comparison_core in list_sites: # loop through each core other than the initial load\n",
    "        pair = f\"{core}-{comparison_core}\"\n",
    "        if core != comparison_core and pair in pairs: # make sure not the same core and we skip non-existent linkages\n",
    "            pair_dir = Path(f'/Users/quinnmackay/Documents/GitHub/BICC/Antarctic Chronology Accuracy Project/{project}/{pair}')\n",
    "\n",
    "            # Check: directory exists AND contains at least one .txt file\n",
    "            txt_files = list(pair_dir.glob(\"*.txt\"))\n",
    "            if not pair_dir.is_dir() or not txt_files:\n",
    "                continue\n",
    "\n",
    "            dfs=[] #load all text files into one\n",
    "            for txt in txt_files:\n",
    "                df = pd.read_csv(txt, sep=\"\\t\", comment=\"#\")\n",
    "                dfs.append(df)\n",
    "    \n",
    "            num_files = len(dfs)\n",
    "            load_data = pd.concat(dfs, ignore_index=True)\n",
    "            original_rows = len(load_data)\n",
    "\n",
    "            drop_rows = []\n",
    "            drop_rows_merge = set()\n",
    "            new_merged_rows = []\n",
    "            for idx, row in load_data.iterrows():\n",
    "\n",
    "                mask1 = abs(row['depth1'] - load_data['depth1']) <= error_margin\n",
    "                mask1[idx] = False\n",
    "                mask2 = abs(row['depth2'] - load_data['depth2']) <= error_margin \n",
    "                mask2[idx] = False\n",
    "\n",
    "                close_points = load_data[mask1 & mask2]\n",
    "                num_close = len(close_points)\n",
    "                close_idxs = load_data.index[mask1 & mask2]\n",
    "\n",
    "                if num_close > 0:\n",
    "                    refs = [load_data.at[idx, 'reference']] + [load_data.at[i, 'reference'] for i in close_idxs] #adjoin references\n",
    "                    merged_ref = \"; \".join(str(r) for r in refs if pd.notna(r))\n",
    "\n",
    "                    depth1_vals = [load_data.at[idx, 'depth1']] + [load_data.at[i, 'depth1'] for i in close_idxs]\n",
    "                    merged_depth1 = np.mean(depth1_vals)\n",
    "\n",
    "                    depth2_vals = [load_data.at[idx, 'depth2']] + [load_data.at[i, 'depth2'] for i in close_idxs]\n",
    "                    merged_depth2 = np.mean(depth2_vals)\n",
    "\n",
    "                    new_merged_rows.append({'reference': merged_ref, 'depth1': merged_depth1, 'depth2': merged_depth2}) #create new merged row\n",
    "\n",
    "                    drop_rows_merge.add(idx)\n",
    "                    for i in close_idxs:\n",
    "                        drop_rows.append(i)\n",
    "                        if drop_rows.count(i) >= num_files:\n",
    "                            print(f'WARNING: Row {load_data.at[i, 'depth1']} | {load_data.at[i, 'depth2']} for {pair}. Reference {load_data.at[i, 'reference']}.')\n",
    "                            print(f'Called by row {load_data.at[idx, 'depth1']} | {load_data.at[idx, 'depth2']} from reference {load_data.at[idx, 'reference']}.')\n",
    "\n",
    "            # drop duplicate rows\n",
    "            drop_rows = set(drop_rows).union(drop_rows_merge)\n",
    "            load_data = load_data.drop(index=drop_rows).reset_index(drop=True)\n",
    "            # add merged rows\n",
    "            merged_df = pd.DataFrame(new_merged_rows)\n",
    "            load_data = pd.concat([load_data, merged_df], ignore_index=True)\n",
    "            load_data.drop_duplicates(subset=['depth1', 'depth2'], inplace=True)\n",
    "            load_data = load_data.reset_index(drop=True)\n",
    "\n",
    "            load_data = load_data.sort_values(by=['depth1']).reset_index(drop=True)\n",
    "\n",
    "            # rename to create unique columns for this pair\n",
    "            load_data = load_data.rename(columns={\n",
    "                'depth1': f\"{pair}_{core}\",\n",
    "                'depth2': f\"{pair}_{comparison_core}\",\n",
    "                'reference': f\"{pair}_reference\"\n",
    "            })\n",
    "\n",
    "            print(f\"Processed pair {pair}, total points after merging: {len(load_data)}, ({original_rows} original total rows)\")\n",
    "            # append rows (block)\n",
    "            big_table = pd.concat([big_table, load_data],\n",
    "                                  axis=0,\n",
    "                                  ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total updates made: 19461 (+19461)\n",
      "total updates made: 20204 (+743)\n",
      "total updates made: 20238 (+34)\n",
      "total updates made: 20238 (+0)\n",
      "Reduced table by 68.89% due to duplicates\n",
      "Identified rows with small within-row errors, 92 total\n",
      "Identified rows with big within-row errors, 9 total\n",
      "Renamed all columns to their suffix\n",
      "Exported cleaned table to excel at /Users/quinnmackay/Desktop/table_out/Greenland_full.xlsx\n",
      "Exported 3+ filtered table to excel at /Users/quinnmackay/Desktop/table_out/Greenland_2plus.xlsx\n"
     ]
    }
   ],
   "source": [
    "from numpy._core.numeric import indices\n",
    "\n",
    "\n",
    "core_groups = defaultdict(list)\n",
    "matching_groups = defaultdict(list)\n",
    "\n",
    "link_columns = [col for col in big_table.columns if \"reference\" not in col]\n",
    "\n",
    "for col in link_columns:\n",
    "    suffix = col.split(\"_\")[-1]\n",
    "    core_groups[suffix].append(col) #group cols by suffix\n",
    "    \n",
    "    match = col.split(\"_\")[0]\n",
    "    core1 = match.split(\"-\")[0]\n",
    "    core2 = match.split(\"-\")[1]\n",
    "\n",
    "    if core1 == suffix:\n",
    "        matching_core = core2\n",
    "    elif core2 == suffix:\n",
    "        matching_core = core1\n",
    "\n",
    "    matching_groups[suffix].append(f\"{match}_{matching_core}\")\n",
    "\n",
    "update_check = 0  # total number of filled-in values across all passes\n",
    "refresh = 1  # triggers loop until no new updates are found\n",
    "while refresh > 0:  # keep looping as long as new values were added\n",
    "    refresh = 0  # reset per loop\n",
    "    for core, assoc_cols in core_groups.items():  # columns associated with each core\n",
    "        matching_cols = matching_groups[core]  # corresponding matching columns\n",
    "\n",
    "        for col, match_col in zip(assoc_cols, matching_cols):  # pair actual vs matching column\n",
    "            base_col = col.split(\"_\")[0] # get base link name for reference column\n",
    "            ref_col = f\"{base_col}_reference\"  # reference column name\n",
    "            for col_check in assoc_cols:  # compare against all other columns of same core\n",
    "                if col == col_check:  # skip self-comparison\n",
    "                    continue\n",
    "\n",
    "                col_updates = {}  # values to update in primary column\n",
    "                match_updates = {}  # values to update in matching column\n",
    "                ref_updates = {}  # values to update in reference column\n",
    "\n",
    "                for index, value in big_table[col].items():  # loop over each row\n",
    "                    diff = (big_table[col_check] - value).abs()  # compute absolute diff\n",
    "                    matching_indices = diff[diff <= error_margin].index  # rows that agree within margin\n",
    "\n",
    "                    for match_idx in matching_indices:  # for each compatible row\n",
    "                        col_updates[match_idx] = big_table[col].at[index]  # schedule update for col\n",
    "                        match_updates[match_idx] = big_table[match_col].at[index]  # schedule update for match_col\n",
    "                        ref_updates[match_idx] = big_table[ref_col].at[index]  # schedule update for reference column\n",
    "            \n",
    "                for match_idx, new_val in col_updates.items():  # apply col updates\n",
    "                    if pd.isna(big_table.at[match_idx, col]):  # only fill empty cells\n",
    "                        big_table.at[match_idx, col] = new_val  # write new value\n",
    "                        update_check+=1  # count total updates\n",
    "                        refresh+=1  # signal another full loop is needed\n",
    "                for match_idx, new_val in match_updates.items():  # apply match_col updates\n",
    "                    if pd.isna(big_table.at[match_idx, match_col]):  # only fill empty cells\n",
    "                        big_table.at[match_idx, match_col] = new_val  # write new value\n",
    "                for match_idx, new_val in ref_updates.items():  # apply reference column updates\n",
    "                    if pd.isna(big_table.at[match_idx, ref_col]):  # only fill empty cells\n",
    "                        big_table.at[match_idx, ref_col] = new_val  # write new value\n",
    "            \n",
    "    print(f'total updates made: {update_check} (+{refresh})')  # show total and new updates this pass\n",
    "\n",
    "#deal with with duplicates\n",
    "non_ref_cols = [c for c in big_table.columns if \"reference\" not in c]\n",
    "big_table[non_ref_cols] = big_table[non_ref_cols].round(8)\n",
    "duplicates_mask = big_table.duplicated(subset=non_ref_cols, keep='first')\n",
    "num_dupe = duplicates_mask.sum()\n",
    "big_table_cleaned = big_table.drop_duplicates(subset=non_ref_cols, keep='first').reset_index(drop=True)\n",
    "print(f'Reduced table by {num_dupe/len(big_table)*100:.2f}% due to duplicates')\n",
    "\n",
    "#reorganize based on ages\n",
    "core_chron = {}\n",
    "for core, columns in core_groups.items():\n",
    "    file_path = f'/Users/quinnmackay/Documents/GitHub/BICC/Antarctic Chronology Accuracy Project/{project}/Chronologies/{core}.txt'\n",
    "    df = pd.read_csv(file_path, sep=\"\\t\", comment=\"#\", names=['depth', 'age']).sort_values(by=['depth']).reset_index(drop=True)\n",
    "    core_chron[core] = df\n",
    "\n",
    "for index, row in big_table_cleaned.iterrows():\n",
    "    for core, columns in core_groups.items(): # For each core and its associated list of column names\n",
    "        if core != base_core_age:\n",
    "            continue\n",
    "# for index, row in big_table_cleaned.iterrows():\n",
    "#     row_age=[]\n",
    "#     for core, columns in core_groups.items(): # For each core and its associated list of column names\n",
    "#         values = [] # Collect the values for this core on this row\n",
    "#         values = [row[col] for col in columns if not pd.isna(row[col])] # Remove NaN values so they don't interfere with comparison, get values for this core on this row\n",
    "#         if len(values) >= 1:\n",
    "#             avg_depth = np.mean(values)\n",
    "#             chron_df = core_chron[core]\n",
    "#             age_core_row = np.interp(avg_depth, chron_df['depth'], chron_df['age'])\n",
    "#             row_age.append(age_core_row)\n",
    "#     avg_row_age = np.mean(row_age)\n",
    "#     big_table_cleaned.at[index, 'estimated_age'] = avg_row_age\n",
    "\n",
    "# big_table_cleaned = big_table_cleaned.sort_values(by=['estimated_age']).reset_index(drop=True)\n",
    "# big_table_cleaned = big_table_cleaned.drop(columns=['estimated_age'])\n",
    "\n",
    "\n",
    "# Do evaluation for errors (inter-row errors)\n",
    "within_row_errors = []\n",
    "within_row_errors_core = []\n",
    "within_row_big_errors = []\n",
    "within_row_big_errors_core = []\n",
    "for index, row in big_table_cleaned.iterrows(): # Iterate over every row in the table\n",
    "    for core, columns in core_groups.items(): # For each core and its associated list of column names\n",
    "        values = []\n",
    "        for col in columns: # Collect the values for this core on this row\n",
    "            values.append(row[col])\n",
    "        values = [v for v in values if not pd.isna(v)] # Remove NaN values so they don't interfere with comparison\n",
    "        if len(values) >= 2:\n",
    "            diff = abs(max(values) - min(values))\n",
    "            if diff >= error_margin:\n",
    "                within_row_errors.append(index)\n",
    "                within_row_errors_core.append(core)\n",
    "                #print(f\"Row {index} core {core} values: {values} diff={diff}\")\n",
    "            if diff >= big_error_margin and index not in within_row_big_errors:\n",
    "                within_row_big_errors.append(index)\n",
    "print(f'Identified rows with small within-row errors, {len(within_row_errors) - len(within_row_big_errors)} total')\n",
    "print(f'Identified rows with big within-row errors, {len(within_row_big_errors)} total')\n",
    "\n",
    "# # Eval between row errors\n",
    "# for index, row in big_table_cleaned.iterrows(): # Iterate over every row in the table\n",
    "#     if index not in within_row_errors:\n",
    "#         continue\n",
    "#     core_grab = [within_row_errors_core[within_row_errors.index(index)]]\n",
    "#     between_indices = [i for i, v in enumerate(within_row_errors) if v == index]\n",
    "#     cores = [within_row_errors_core[i] for i in between_indices]\n",
    "#     for core, columns in core_groups.items(): # For each core and its associated list of column names\n",
    "#         if core not in cores:\n",
    "#             continue\n",
    "        \n",
    "\n",
    "#move cols around\n",
    "reference_cols = [c for c in big_table_cleaned.columns if \"reference\" in c]\n",
    "other_cols = [c for c in big_table_cleaned.columns if \"reference\" not in c]\n",
    "big_table_cleaned = big_table_cleaned[other_cols + reference_cols]\n",
    "\n",
    "rename_map = {}\n",
    "for suffix, cols in core_groups.items():\n",
    "    for col in cols:\n",
    "        rename_map[col] = suffix  # rename to suffix only\n",
    "big_table_cleaned.rename(columns=rename_map, inplace=True)\n",
    "print('Renamed all columns to their suffix')\n",
    "\n",
    "index_v = True\n",
    "min_cols_per = {}\n",
    "\n",
    "min_cols_export = 0\n",
    "excel_path = f'{output_dir}/{project}_full.xlsx'\n",
    "big_table_cleaned = big_table_cleaned[big_table_cleaned.notna().sum(axis=1) >= min_cols_export]\n",
    "big_table_cleaned.to_excel(excel_path, index=index_v)\n",
    "min_cols_per[excel_path] = min_cols_export\n",
    "print(f'Exported cleaned table to excel at {excel_path}')\n",
    "\n",
    "min_pairs_export = 2\n",
    "excel_path = f'{output_dir}/{project}_{min_pairs_export}plus.xlsx'\n",
    "min_cols_export = ((min_pairs_export-1) * 3) + 1 # y = x - 1 (so its minimum cols above the max of 1 less pair), times 3 cols per pair, plus 1 to be above\n",
    "filtered_big_table = big_table_cleaned[big_table_cleaned.notna().sum(axis=1) >= min_cols_export]\n",
    "filtered_big_table.to_excel(excel_path, index=index_v)\n",
    "min_cols_per[excel_path] = min_pairs_export\n",
    "print(f'Exported 3+ filtered table to excel at {excel_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded workbook /Users/quinnmackay/Desktop/table_out/Greenland_full.xlsx for styling\n",
      "Added error corrections for Greenland_full.xlsx\n",
      "Styled and saved workbook at /Users/quinnmackay/Desktop/table_out/Greenland_full.xlsx\n",
      "Loaded workbook /Users/quinnmackay/Desktop/table_out/Greenland_2plus.xlsx for styling\n",
      "Added error corrections for Greenland_2plus.xlsx\n",
      "Styled and saved workbook at /Users/quinnmackay/Desktop/table_out/Greenland_2plus.xlsx\n"
     ]
    }
   ],
   "source": [
    "excel_paths = list(min_cols_per.keys())\n",
    "\n",
    "#thick side\n",
    "thick_side = Side(border_style=\"thick\", color=\"000000\") \n",
    "thick_left_border = Border(left=thick_side)\n",
    "medium_side = Side(border_style=\"medium\", color=\"000000\")\n",
    "bottom_medium_border = Border(bottom=medium_side)\n",
    "medium_left_border = Border(left=medium_side)\n",
    "medium_right_border = Border(right=medium_side)\n",
    "medium_border = Border(top=medium_side, left=medium_side, right=medium_side, bottom=medium_side)\n",
    "light_side = Side(border_style=\"thin\", color=\"000000\")\n",
    "light_left_border = Border(left=light_side)\n",
    "\n",
    "for excel_path in excel_paths:\n",
    "    wb = load_workbook(excel_path)\n",
    "    ws = wb.active\n",
    "    print(f\"Loaded workbook {excel_path} for styling\")\n",
    "\n",
    "    #create second page for legend/stats\n",
    "    legend_sheet = wb.create_sheet(title=\"Legend, Stats, and References\")\n",
    "    legend_sheet[\"A1\"] = \"Legend and Stats\"\n",
    "    legend_sheet[\"A1\"].font = Font(size=14, bold=True)\n",
    "    legend_sheet[\"A1\"].alignment = Alignment(horizontal=\"center\")\n",
    "    legend_sheet.merge_cells('A1:D1')  # Merge first row for title\n",
    "    for col_idx in range(1, 5):\n",
    "        legend_sheet.cell(row=1, column=col_idx).border = medium_border\n",
    "    legend_sheet.freeze_panes = \"A2\"\n",
    "\n",
    "    #### Legend sheet reference move\n",
    "\n",
    "    # Find all columns with 'reference' in header (case-insensitive)\n",
    "    reference_cols = [i+1 for i, cell in enumerate(ws[1]) if cell.value and \"reference\" in str(cell.value).lower()]\n",
    "\n",
    "    if reference_cols:\n",
    "        # Copy index column header and data (assumes index col is 1)\n",
    "        legend_sheet.cell(row=1, column=7).value = \"Index\"\n",
    "        legend_sheet.cell(row=1, column=7).font = Font(bold=True)\n",
    "        legend_sheet.cell(row=1, column=7).alignment = Alignment(horizontal=\"center\", vertical=\"center\")\n",
    "        legend_sheet.cell(row=1, column=7).border = medium_border\n",
    "\n",
    "        for row_idx in range(2, ws.max_row + 1): #index copy\n",
    "            legend_sheet.cell(row=row_idx, column=7).value = ws.cell(row=row_idx, column=1).value\n",
    "            legend_sheet.cell(row=row_idx, column=7).alignment = Alignment(horizontal=\"center\", vertical=\"center\")\n",
    "            legend_sheet.cell(row=row_idx, column=7).border = medium_border\n",
    "            legend_sheet.cell(row=row_idx, column=7).fill = PatternFill(start_color=\"FFFFBA\", end_color=\"FFFFBA\", fill_type='solid')  # light yellow fill\n",
    "        # Copy each reference column header and data starting from column 7 (F=6, so G=7 etc)\n",
    "        #rename reference headers\n",
    "        for offset, col_idx in enumerate(reference_cols, start=8):\n",
    "            original_header = ws.cell(row=1, column=col_idx).value\n",
    "            # Split by '_' and keep the first part\n",
    "            new_header = original_header.split('_')[0] if original_header else \"\"\n",
    "            legend_sheet.cell(row=1, column=offset).value = new_header\n",
    "            legend_sheet.cell(row=1, column=offset).alignment = Alignment(horizontal=\"center\", vertical=\"center\")\n",
    "            legend_sheet.cell(row=1, column=offset).font = Font(bold=True)\n",
    "            legend_sheet.cell(row=1, column=offset).border = bottom_medium_border\n",
    "            \n",
    "            offset_letter = get_column_letter(offset)\n",
    "            legend_sheet.column_dimensions[offset_letter].width = 25\n",
    "            for row_idx in range(2, ws.max_row + 1): #value copy\n",
    "                legend_sheet.cell(row=row_idx, column=offset).value = ws.cell(row=row_idx, column=col_idx).value\n",
    "                legend_sheet.cell(row=row_idx, column=offset).border = light_left_border\n",
    "        # Remove reference columns from main sheet, delete from right to left\n",
    "        for col_idx in sorted(reference_cols, reverse=True):\n",
    "            ws.delete_cols(col_idx)\n",
    "\n",
    "    ####### Legend Dicts\n",
    "\n",
    "    # Add legend entries\n",
    "    legend = {\n",
    "        \"ffd966\": \"Rows flagged with values differing by > 0.1 but all less than < 0.25\",\n",
    "        \"e06666\": \"Rows flagged with values maximum differing by > 0.25\",\n",
    "    }\n",
    "\n",
    "    legend_row = 3\n",
    "    legend_sheet[f\"A{legend_row}\"] = \"Legend\"\n",
    "    legend_sheet[f\"A{legend_row}\"].font = Font(bold=True)\n",
    "\n",
    "    legend_row +=1\n",
    "    for key, desc in legend.items():\n",
    "        cell = legend_sheet[f\"A{legend_row}\"]\n",
    "        cell.fill = PatternFill(start_color=key, end_color=key, fill_type='solid')  # Set fill style\n",
    "        legend_sheet[f\"B{legend_row}\"] = desc\n",
    "        legend_row += 1\n",
    "\n",
    "    #count how many error on this sheet\n",
    "    num_minor = 0\n",
    "    num_major = 0\n",
    "    for row_idx in range(2, ws.max_row + 1):\n",
    "            cell = ws.cell(row=row_idx, column=1)\n",
    "            if cell.value in within_row_errors:\n",
    "                num_minor += 1\n",
    "            if cell.value in within_row_big_errors:\n",
    "                num_major += 1\n",
    "\n",
    "    # Add some stats\n",
    "    stats = {\n",
    "        \"Total Rows\": ws.max_row - 1,  # assuming ws is your main sheet, -1 for header\n",
    "        \"Total Rows with Errors\": f\"{num_minor} ({num_minor / (ws.max_row - 1) * 100:.2f}%)\",\n",
    "        \"Total Minor Errors (excl. Major)\": f\"{num_minor - num_major} ({(num_minor - num_major) / (ws.max_row - 1) * 100:.2f}%)\",\n",
    "        \"Total Major Errors\": f\"{num_major} ({num_major / (ws.max_row - 1) * 100:.2f}%)\",\n",
    "        \"Minimum Columns per Row\": f\"{min_cols_per[excel_path]}\",\n",
    "    }\n",
    "\n",
    "    stats_row = legend_row + len(legend)\n",
    "    legend_sheet[f\"A{stats_row}\"] = \"Statistics\"\n",
    "    legend_sheet[f\"A{stats_row}\"].font = Font(bold=True)\n",
    "\n",
    "    stats_row +=1\n",
    "    for stat, val in stats.items():\n",
    "        legend_sheet[f\"A{stats_row}\"] = stat\n",
    "        legend_sheet[f\"B{stats_row}\"] = val\n",
    "        stats_row += 1\n",
    "\n",
    "    #adjust readability\n",
    "    legend_sheet.column_dimensions['A'].width = 25  # wider column A in legend\n",
    "    legend_sheet.column_dimensions['B'].width = 25  # wider column A in legend\n",
    "\n",
    "    ### Styling main sheet ----------\n",
    "\n",
    "    # Load headers\n",
    "    headers = [cell.value for cell in ws[1]]\n",
    "\n",
    "   # error coloring and error column\n",
    "    max_columns = ws.max_column\n",
    "    if index_v:\n",
    "        headers_to_color = headers[1:]\n",
    "        start_col = 2   # Excel column index: 1 = index col, 2 = real col 1\n",
    "        ws[\"A1\"].value = \"Index\"\n",
    "        ws[\"A1\"].font = Font(bold=True)\n",
    "        ws[\"A1\"].alignment = Alignment(horizontal=\"center\", vertical=\"center\")\n",
    "\n",
    "        for row_idx in range(2, ws.max_row + 1):\n",
    "            cell = ws.cell(row=row_idx, column=1)\n",
    "            cell.font = Font(bold=False)\n",
    "            cell.alignment = Alignment(horizontal=\"center\", vertical=\"center\")\n",
    "            if cell.value in within_row_errors:\n",
    "                cell.fill = PatternFill(start_color=\"ffd966\", end_color=\"ffd966\", fill_type='solid')\n",
    "\n",
    "                error_cell = ws.cell(row=row_idx, column=max_columns + 1)\n",
    "                indices = [i for i, v in enumerate(within_row_errors) if v == cell.value]\n",
    "                cores = [within_row_errors_core[i] for i in indices]\n",
    "                error_cell.value = \", \".join(cores)\n",
    "\n",
    "            if cell.value in within_row_big_errors:\n",
    "                cell.fill = PatternFill(start_color=\"e06666\", end_color=\"e06666\", fill_type='solid')\n",
    "        error_column = get_column_letter(max_columns + 1)\n",
    "        ws.column_dimensions[error_column].width = 25\n",
    "        print(f'Added error corrections for {os.path.basename(excel_path)}')\n",
    "\n",
    "    else:\n",
    "        headers_to_color = headers\n",
    "        start_col = 1\n",
    "\n",
    "    # Define distinct light colors\n",
    "    colors = [\n",
    "        \"FFB3BA\", \"FFDFBA\", \"FFFFBA\", \"BAFFC9\", \"BAE1FF\",\n",
    "        \"D7BAFF\", \"FFC3F7\", \"BAFFD9\", \"FFE0BA\", \"D0BAFF\"\n",
    "    ]\n",
    "\n",
    "    colors_note = [\n",
    "    \"#FFB3BA\", \"#FFDFBA\", \"#FFFFBA\", \"#BAFFC9\", \"#BAE1FF\",\n",
    "    \"#D7BAFF\", \"#FFC3F7\", \"#BAFFD9\", \"#FFE0BA\", \"#D0BAFF\"]\n",
    "\n",
    "    # Assign colors to unique header names\n",
    "    color_map = {}\n",
    "    for col_name in headers_to_color:\n",
    "        if col_name not in color_map:\n",
    "            color_map[col_name] = colors[len(color_map) % len(colors)]\n",
    "\n",
    "    # Apply fill color to each column\n",
    "    col_idx = start_col\n",
    "    for col_name in headers_to_color:\n",
    "\n",
    "        fill = PatternFill(\n",
    "            start_color=color_map[col_name],\n",
    "            end_color=color_map[col_name],\n",
    "            fill_type='solid'\n",
    "        )\n",
    "\n",
    "        # Color header\n",
    "        ws.cell(row=1, column=col_idx).fill = fill\n",
    "\n",
    "        # Color all data rows\n",
    "        for row_idx in range(2, ws.max_row + 1):\n",
    "            ws.cell(row=row_idx, column=col_idx).fill = fill\n",
    "\n",
    "        col_idx += 1\n",
    "\n",
    "    # Loop through columns starting at column 3 (Excel index), applying thick border every 2 columns \n",
    "    for col_idx in range(2, ws.max_column + 2, 2): # 3, 5, 7, 9 ... \n",
    "        for row_idx in range(1, ws.max_row + 1):\n",
    "            ws.cell(row=row_idx, column=col_idx).border = thick_left_border\n",
    "\n",
    "    # Medium thick on bottom of row 1\n",
    "    medium_border_int = 1 if index_v else 0\n",
    "    for col_idx in range(1, ws.max_column + medium_border_int):\n",
    "        ws.cell(row=1, column=col_idx).border = bottom_medium_border\n",
    "\n",
    "    #freeze top row\n",
    "    ws.freeze_panes = \"A2\"\n",
    "\n",
    "    #add error column header\n",
    "    if index_v:\n",
    "        error_col_cell = ws.cell(row=1, column=max_columns + 1)\n",
    "        error_col_cell.value = \"Error Cores\"\n",
    "        error_col_cell.font = Font(bold=True)\n",
    "        error_col_cell.alignment = Alignment(horizontal=\"center\", vertical=\"center\")\n",
    "\n",
    "    #rename sheet\n",
    "    ws.title = \"Ice Core Depth Comparison\"\n",
    "\n",
    "    # Save workbook\n",
    "    wb.save(excel_path)\n",
    "    print(f\"Styled and saved workbook at {excel_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bicc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
