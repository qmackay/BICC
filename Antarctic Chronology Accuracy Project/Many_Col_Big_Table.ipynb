{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import os\n",
    "import yaml\n",
    "import itertools\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from openpyxl.styles import PatternFill\n",
    "from openpyxl import load_workbook\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "from openpyxl.styles import Border, Side\n",
    "from openpyxl.styles import Alignment, Font\n",
    "from openpyxl.utils import get_column_letter\n",
    "from numpy._core.numeric import indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cores list of sites for AA\n",
    "\n",
    "project = 'Antarctic'\n",
    "output_dir = 'table_out/'\n",
    "\n",
    "# get all link combos\n",
    "with open(f'/Users/quinnmackay/Documents/GitHub/BICC/Antarctic Chronology Accuracy Project/{project}/parameters.yml') as f:\n",
    "    data = yaml.safe_load(f)\n",
    "list_sites = data[\"list_sites\"]\n",
    "pairs = [f\"{a}-{b}\" for a, b in itertools.combinations(list_sites, 2)]\n",
    "\n",
    "error_margin = 0.15\n",
    "big_error_margin = 0.25\n",
    "minor_depth_inversion = 0.001\n",
    "major_depth_inversion = 0.5\n",
    "base_core_age = 'EDC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed pair EDC-WDC, total points after merging: 642, (897 original total rows)\n",
      "WARNING: Row 791.88 | 1454.37 for EDC-EDML. Reference AICC2012_Links.\n",
      "Called by row 791.88 | 1454.356 from reference Svensson_Links.\n",
      "WARNING: Row 791.874 | 1454.36 for EDC-EDML. Reference Severi2007_Ruth2007.\n",
      "Called by row 791.88 | 1454.37 from reference AICC2012_Links.\n",
      "WARNING: Row 791.88 | 1454.37 for EDC-EDML. Reference Severi2007_Ruth2007.\n",
      "Called by row 791.88 | 1454.37 from reference AICC2012_Links.\n",
      "WARNING: Row 791.88 | 1454.356 for EDC-EDML. Reference Svensson_Links.\n",
      "Called by row 791.88 | 1454.37 from reference AICC2012_Links.\n",
      "Processed pair EDC-EDML, total points after merging: 488, (828 original total rows)\n",
      "WARNING: Row 1388.89 | 1486.02 for EDC-DF. Reference Svensson_Links.\n",
      "Called by row 1388.94 | 1486.13 from reference Fujita2015.\n",
      "WARNING: Row 1388.88 | 1486.02 for EDC-DF. Reference Fujita2015.\n",
      "Called by row 1388.89 | 1486.02 from reference Svensson_Links.\n",
      "WARNING: Row 1388.94 | 1486.13 for EDC-DF. Reference Fujita2015.\n",
      "Called by row 1388.89 | 1486.02 from reference Svensson_Links.\n",
      "Processed pair EDC-DF, total points after merging: 1442, (1605 original total rows)\n",
      "Processed pair EDC-TALDICE, total points after merging: 203, (246 original total rows)\n",
      "Processed pair WDC-EDML, total points after merging: 978, (1315 original total rows)\n",
      "Processed pair WDC-DF, total points after merging: 613, (1078 original total rows)\n",
      "Processed pair WDC-TALDICE, total points after merging: 662, (1219 original total rows)\n",
      "Processed pair EDML-DF, total points after merging: 215, (215 original total rows)\n",
      "Processed pair EDML-TALDICE, total points after merging: 128, (128 original total rows)\n",
      "Processed pair DF-TALDICE, total points after merging: 111, (111 original total rows)\n"
     ]
    }
   ],
   "source": [
    "big_table = pd.DataFrame()\n",
    "all_links_count = {}\n",
    "all_links_foragesort = {}\n",
    "all_links_total = {}\n",
    "\n",
    "if base_core_age not in list_sites:\n",
    "    print('Incorrect base_core set for sorting')\n",
    "    sys.exit()\n",
    "\n",
    "for core in list_sites: # loop through each core\n",
    "    for comparison_core in list_sites: # loop through each core other than the initial load\n",
    "        pair = f\"{core}-{comparison_core}\"\n",
    "        if core != comparison_core and pair in pairs: # make sure not the same core and we skip non-existent linkages\n",
    "            pair_dir = Path(f'/Users/quinnmackay/Documents/GitHub/BICC/Antarctic Chronology Accuracy Project/{project}/{pair}')\n",
    "\n",
    "            # Check: directory exists AND contains at least one .txt file\n",
    "            txt_files = list(pair_dir.glob(\"*.txt\"))\n",
    "            if not pair_dir.is_dir() or not txt_files:\n",
    "                continue\n",
    "\n",
    "            dfs=[] #load all text files into one\n",
    "            for txt in txt_files:\n",
    "                df = pd.read_csv(txt, sep=\"\\t\", comment=\"#\")\n",
    "                dfs.append(df)\n",
    "    \n",
    "            num_files = len(dfs)\n",
    "            load_data = pd.concat(dfs, ignore_index=True)\n",
    "            original_rows = len(load_data)\n",
    "\n",
    "            drop_rows = []\n",
    "            drop_rows_merge = set()\n",
    "            new_merged_rows = []\n",
    "            for idx, row in load_data.iterrows():\n",
    "\n",
    "                mask1 = abs(row['depth1'] - load_data['depth1']) <= error_margin\n",
    "                mask1[idx] = False\n",
    "                mask2 = abs(row['depth2'] - load_data['depth2']) <= error_margin \n",
    "                mask2[idx] = False\n",
    "\n",
    "                close_points = load_data[mask1 & mask2]\n",
    "                num_close = len(close_points)\n",
    "                close_idxs = load_data.index[mask1 & mask2]\n",
    "\n",
    "                if num_close > 0:\n",
    "                    refs = [load_data.at[idx, 'reference']] + [load_data.at[i, 'reference'] for i in close_idxs] #adjoin references\n",
    "                    merged_ref = \"; \".join(str(r) for r in refs if pd.notna(r))\n",
    "\n",
    "                    depth1_vals = [load_data.at[idx, 'depth1']] + [load_data.at[i, 'depth1'] for i in close_idxs]\n",
    "                    merged_depth1 = np.mean(depth1_vals)\n",
    "\n",
    "                    depth2_vals = [load_data.at[idx, 'depth2']] + [load_data.at[i, 'depth2'] for i in close_idxs]\n",
    "                    merged_depth2 = np.mean(depth2_vals)\n",
    "\n",
    "                    new_merged_rows.append({'reference': merged_ref, 'depth1': merged_depth1, 'depth2': merged_depth2}) #create new merged row\n",
    "\n",
    "                    drop_rows_merge.add(idx)\n",
    "                    for i in close_idxs:\n",
    "                        drop_rows.append(i)\n",
    "                        if drop_rows.count(i) >= num_files:\n",
    "                            print(f'WARNING: Row {load_data.at[i, 'depth1']} | {load_data.at[i, 'depth2']} for {pair}. Reference {load_data.at[i, 'reference']}.')\n",
    "                            print(f'Called by row {load_data.at[idx, 'depth1']} | {load_data.at[idx, 'depth2']} from reference {load_data.at[idx, 'reference']}.')\n",
    "\n",
    "            # drop duplicate rows\n",
    "            drop_rows = set(drop_rows).union(drop_rows_merge)\n",
    "            load_data = load_data.drop(index=drop_rows).reset_index(drop=True)\n",
    "            # add merged rows\n",
    "            merged_df = pd.DataFrame(new_merged_rows)\n",
    "            load_data = pd.concat([load_data, merged_df], ignore_index=True)\n",
    "            load_data.drop_duplicates(subset=['depth1', 'depth2'], inplace=True)\n",
    "            load_data = load_data.reset_index(drop=True)\n",
    "\n",
    "            load_data = load_data.sort_values(by=['depth1']).reset_index(drop=True)\n",
    "        \n",
    "            #set up pair code stuff\n",
    "            load_data[f\"{pair}_code\"] = [f\"{pair}_{idx}\" for idx in load_data.index]\n",
    "\n",
    "            if core == base_core_age or comparison_core == base_core_age: # get all links attached to base core for age sorting\n",
    "                if core == base_core_age:\n",
    "                    all_links_count[comparison_core] = len(load_data)\n",
    "                    all_links_foragesort[comparison_core] = load_data[['depth1', 'depth2']].copy() #get all links for sorting the age, all links attached to base core\n",
    "                    all_links_foragesort[comparison_core].rename(columns={f'depth1': base_core_age, 'depth2': 'comparison_core'}, inplace=True)\n",
    "                elif comparison_core == base_core_age:\n",
    "                    all_links_count[core] = len(load_data)\n",
    "                    all_links_foragesort[core] = load_data[['depth1', 'depth2']].copy()\n",
    "                    all_links_foragesort[core].rename(columns={f'depth2': base_core_age, 'depth1': 'comparison_core'}, inplace=True)\n",
    "\n",
    "            #save all the links for this pair\n",
    "            all_links_total[f'{pair}'] = load_data[['depth1', 'depth2']].copy(deep=True)\n",
    "            all_links_total[f'{pair}'] = all_links_total[f'{pair}'].rename(columns={\n",
    "                'depth1': pair.split(\"-\")[0],\n",
    "                'depth2': pair.split(\"-\")[1]})\n",
    "\n",
    "            # rename to create unique columns for this pair\n",
    "            load_data = load_data.rename(columns={\n",
    "                'depth1': f\"{pair}_{core}\",\n",
    "                'depth2': f\"{pair}_{comparison_core}\",\n",
    "                'reference': f\"{pair}_reference\",\n",
    "            })\n",
    "\n",
    "            print(f\"Processed pair {pair}, total points after merging: {len(load_data)}, ({original_rows} original total rows)\")\n",
    "            # append rows (block)\n",
    "            big_table = pd.concat([big_table, load_data],\n",
    "                                  axis=0,\n",
    "                                  ignore_index=True)\n",
    "\n",
    "# make all_links_count the largest value\n",
    "all_links_count[base_core_age] = sum(all_links_count.values())\n",
    "\n",
    "#if core doesn't exist in all_links_count, add it with 0 val\n",
    "for core in list_sites:\n",
    "    if core not in all_links_count:\n",
    "        all_links_count[core] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total updates made: 18924 (+18924)\n",
      "total updates made: 20085 (+1161)\n",
      "total updates made: 20096 (+11)\n",
      "total updates made: 20096 (+0)\n",
      "Reduced table by 53.30% due to duplicates\n"
     ]
    }
   ],
   "source": [
    "core_groups = defaultdict(list)\n",
    "matching_groups = defaultdict(list)\n",
    "\n",
    "link_columns_temp = [col for col in big_table.columns if \"reference\" not in col] #remove ref cols\n",
    "link_columns = [col for col in link_columns_temp if \"code\" not in col] #remove code cols\n",
    "\n",
    "for col in link_columns:\n",
    "    suffix = col.split(\"_\")[-1]\n",
    "    core_groups[suffix].append(col) #group cols by suffix\n",
    "    \n",
    "    match = col.split(\"_\")[0]\n",
    "    core1 = match.split(\"-\")[0]\n",
    "    core2 = match.split(\"-\")[1]\n",
    "\n",
    "    if core1 == suffix:\n",
    "        matching_core = core2\n",
    "    elif core2 == suffix:\n",
    "        matching_core = core1\n",
    "\n",
    "    matching_groups[suffix].append(f\"{match}_{matching_core}\")\n",
    "\n",
    "update_check = 0  # total number of filled-in values across all passes\n",
    "refresh = 1  # triggers loop until no new updates are found\n",
    "while refresh > 0:  # keep looping as long as new values were added\n",
    "    refresh = 0  # reset per loop\n",
    "    for core, assoc_cols in core_groups.items():  # columns associated with each core\n",
    "        matching_cols = matching_groups[core]  # corresponding matching columns\n",
    "\n",
    "        for col, match_col in zip(assoc_cols, matching_cols):  # pair actual vs matching column\n",
    "            base_col = col.split(\"_\")[0] # get base link name for reference column\n",
    "            i_core1 = base_col.split(\"-\")[0] #get core names for index updates\n",
    "            i_core2 = base_col.split(\"-\")[1]\n",
    "            ref_col = f\"{base_col}_reference\"  # reference column name\n",
    "            code_col = f\"{base_col}_code\"\n",
    "            for col_check in assoc_cols:  # compare against all other columns of same core\n",
    "                if col == col_check:  # skip self-comparison\n",
    "                    continue\n",
    "\n",
    "                col_updates = {}  # values to update in primary column\n",
    "                match_updates = {}  # values to update in matching column\n",
    "                ref_updates = {}  # values to update in reference column\n",
    "                code_updates = {}\n",
    "\n",
    "                for index, value in big_table[col].items():  # loop over each row\n",
    "                    try:\n",
    "                        diff = (big_table[col_check] - value).abs()  # compute absolute diff\n",
    "                    except:\n",
    "                        print(f'Error computing diff for {col} and {col_check} at index {index} with value {value}')\n",
    "                        print(core_groups.items())\n",
    "                        sys.exit()\n",
    "                    matching_indices = diff[diff <= error_margin].index  # rows that agree within margin\n",
    "\n",
    "                    for match_idx in matching_indices:  # for each compatible row\n",
    "                        col_updates[match_idx] = big_table[col].at[index]  # schedule update for col\n",
    "                        match_updates[match_idx] = big_table[match_col].at[index]  # schedule update for match_col\n",
    "                        ref_updates[match_idx] = big_table[ref_col].at[index]  # schedule update for reference column\n",
    "                        code_updates[match_idx] = big_table[code_col].at[index]\n",
    "            \n",
    "                for match_idx, new_val in col_updates.items():  # apply col updates\n",
    "                    if pd.isna(big_table.at[match_idx, col]):  # only fill empty cells\n",
    "                        big_table.at[match_idx, col] = new_val  # write new value\n",
    "                        update_check+=1  # count total updates\n",
    "                        refresh+=1  # signal another full loop is needed\n",
    "                for match_idx, new_val in match_updates.items():  # apply match_col updates\n",
    "                    if pd.isna(big_table.at[match_idx, match_col]):  # only fill empty cells\n",
    "                        big_table.at[match_idx, match_col] = new_val  # write new value\n",
    "                for match_idx, new_val in ref_updates.items():  # apply reference column updates\n",
    "                    if pd.isna(big_table.at[match_idx, ref_col]):  # only fill empty cells\n",
    "                        big_table.at[match_idx, ref_col] = new_val  # write new value\n",
    "                for match_idx, new_val in code_updates.items():  # apply reference column updates\n",
    "                    if pd.isna(big_table.at[match_idx, code_col]):  # only fill empty cells\n",
    "                        big_table.at[match_idx, code_col] = new_val  # write new value\n",
    "            \n",
    "    print(f'total updates made: {update_check} (+{refresh})')  # show total and new updates this pass\n",
    "\n",
    "#deal with with duplicates\n",
    "non_ref_code_cols = [c for c in big_table.columns if \"reference\" not in c and \"code\" not in c]\n",
    "big_table[non_ref_code_cols] = big_table[non_ref_code_cols].round(8)\n",
    "duplicates_mask = big_table.duplicated(subset=non_ref_code_cols, keep='first')\n",
    "num_dupe = duplicates_mask.sum()\n",
    "big_table_cleaned = big_table.drop_duplicates(subset=non_ref_code_cols, keep='first').reset_index(drop=True)\n",
    "print(f'Reduced table by {num_dupe/len(big_table)*100:.2f}% due to duplicates')\n",
    "\n",
    "#reorganize based on ages\n",
    "core_chron = {}\n",
    "for core, columns in core_groups.items():\n",
    "    file_path = f'/Users/quinnmackay/Documents/GitHub/BICC/Antarctic Chronology Accuracy Project/{project}/Chronologies/{core}.txt'\n",
    "    df = pd.read_csv(file_path, sep=\"\\t\", comment=\"#\", names=['depth', 'age']).sort_values(by=['depth']).reset_index(drop=True)\n",
    "    core_chron[core] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reorganized table based on estimated ages from EDC\n",
      "More than 1 degree of separation from EDC in set(). Own chronology used, I should probably update in future.\n",
      "Identified rows with small within-row errors, 143 total\n",
      "Identified rows with big within-row errors, 142 total\n",
      "Identified rows with minor depth inversion errors, 152 total\n",
      "Identified rows with major depth inversion errors, 74 total\n",
      "Renamed all columns to their suffix\n",
      "Exported cleaned table to excel at table_out//Antarctic_full.xlsx\n",
      "Exported filtered table to excel at table_out//Antarctic_2plus.xlsx\n"
     ]
    }
   ],
   "source": [
    "chron_df = core_chron[base_core_age]\n",
    "failures = set()\n",
    "for index, row in big_table_cleaned.iterrows():\n",
    "    for core, columns in sorted(core_groups.items(),\n",
    "                                key=lambda x: all_links_count[x[0]],\n",
    "                                reverse=True): # For each core and its associated list of column names\n",
    "        values = [] # Collect the values for this core on this row\n",
    "        values = [row[col] for col in columns if not pd.isna(row[col])] # Remove NaN values so they don't interfere with comparison, get values for this core on this row\n",
    "        if len(values) >= 1 and core == base_core_age:\n",
    "            avg_depth = np.mean(values)\n",
    "            age_core_row = np.interp(avg_depth, chron_df['depth'], chron_df['age'])\n",
    "            big_table_cleaned.at[index, 'estimated_age'] = age_core_row\n",
    "        elif all_links_count[core] == 0:\n",
    "            for next_best_core, count in sorted(all_links_count.items(),key=lambda x: x[1],reverse=True): #go through links to the main core, from most to least links\n",
    "                if os.path.isdir(f\"/Users/quinnmackay/Documents/GitHub/BICC/Antarctic Chronology Accuracy Project/{project}/{core}-{next_best_core}\") or os.path.isdir(f\"/Users/quinnmackay/Documents/GitHub/BICC/Antarctic Chronology Accuracy Project/{project}/{next_best_core}-{core}\"):\n",
    "                    avg_depth = np.mean(values)\n",
    "                    try:\n",
    "                        pair1_interp = \"-\".join(sorted((core, next_best_core), key=list_sites.index)) # get the string name\n",
    "                        equiv_depth1 = np.interp(avg_depth, all_links_total[pair1_interp][core], all_links_total[pair1_interp][next_best_core])\n",
    "                        equiv_depth2 = np.interp(equiv_depth1, all_links_foragesort[next_best_core]['comparison_core'], all_links_foragesort[next_best_core][base_core_age])\n",
    "                        age_core_row = np.interp(equiv_depth2, chron_df['depth'], chron_df['age'])\n",
    "                        big_table_cleaned.at[index, 'estimated_age'] = age_core_row\n",
    "                    except Exception as e:\n",
    "                        failures.add(core)\n",
    "                        failure_chron = core_chron[core]\n",
    "                        age_core_row = np.interp(avg_depth, chron_df['depth'], chron_df['age'])\n",
    "                    break\n",
    "                else:\n",
    "                    continue\n",
    "        elif len(values) >= 1 and pd.isna(big_table_cleaned.at[index, 'estimated_age']): #skip ones already done \n",
    "            avg_depth = np.mean(values)\n",
    "            equiv_depth = np.interp(avg_depth, all_links_foragesort[core]['comparison_core'], all_links_foragesort[core][base_core_age])\n",
    "            age_core_row = np.interp(equiv_depth, chron_df['depth'], chron_df['age'])\n",
    "            big_table_cleaned.at[index, 'estimated_age'] = age_core_row\n",
    "\n",
    "big_table_cleaned = big_table_cleaned.sort_values(by=['estimated_age']).reset_index(drop=True)\n",
    "big_table_cleaned = big_table_cleaned.drop(columns=['estimated_age'])\n",
    "print(f'Reorganized table based on estimated ages from {base_core_age}')\n",
    "print(f\"More than 1 degree of separation from {base_core_age} in {failures}. Own chronology used, I should probably update in future.\")\n",
    "\n",
    "# Do evaluation for errors (inter-row errors)\n",
    "within_row_errors = []\n",
    "within_row_errors_core = []\n",
    "within_row_big_errors = []\n",
    "within_row_big_errors_core = []\n",
    "for index, row in big_table_cleaned.iterrows(): # Iterate over every row in the table\n",
    "    for core, columns in core_groups.items(): # For each core and its associated list of column names\n",
    "        values = []\n",
    "        for col in columns: # Collect the values for this core on this row\n",
    "            values.append(row[col])\n",
    "        values = [v for v in values if not pd.isna(v)] # Remove NaN values so they don't interfere with comparison\n",
    "        if len(values) >= 2:\n",
    "            diff = abs(max(values) - min(values))\n",
    "            if diff >= error_margin:\n",
    "                within_row_errors.append(index)\n",
    "                within_row_errors_core.append(core)\n",
    "                #print(f\"Row {index} core {core} values: {values} diff={diff}\")\n",
    "            if diff >= big_error_margin and index not in within_row_big_errors:\n",
    "                within_row_big_errors.append(index)\n",
    "print(f'Identified rows with small within-row errors, {len(within_row_errors) - len(within_row_big_errors)} total')\n",
    "print(f'Identified rows with big within-row errors, {len(within_row_big_errors)} total')      \n",
    "\n",
    "# Do evaluation for depth inversion errors \n",
    "depth_inversion_errors = []\n",
    "depth_inversion_errors_core = []\n",
    "depth_inversion_errors_major = []\n",
    "for core, columns in core_groups.items():  \n",
    "    for col in columns:  # Check each depth column for this core\n",
    "        # Extract the column's non-NaN values with their index\n",
    "        series = big_table_cleaned[col].dropna()\n",
    "        # Compare each depth with the previous one in sorted index order\n",
    "        inversion_detect = False\n",
    "        last_clean = series.iloc[0]  # Initialize last clean depth value\n",
    "        for (prev_idx, prev_val), (curr_idx, curr_val) in zip(series.items(), list(series.items())[1:]):\n",
    "            \n",
    "            if curr_val > prev_val and inversion_detect==False:\n",
    "                last_clean = curr_val\n",
    "\n",
    "            if inversion_detect==True:\n",
    "                if curr_val >= prev_val and curr_val >= last_clean:\n",
    "                    inversion_detect=False\n",
    "                    last_clean = curr_val\n",
    "                elif curr_val >= prev_val and curr_val < last_clean-minor_depth_inversion: #capture still inverted values just above previous error\n",
    "                    depth_inversion_errors.append(curr_idx)\n",
    "                    depth_inversion_errors_core.append(core)\n",
    "            \n",
    "            if curr_val < prev_val-minor_depth_inversion:  # inversion detected\n",
    "                depth_inversion_errors.append(curr_idx)\n",
    "                depth_inversion_errors_core.append(core)\n",
    "                inversion_detect=True\n",
    "\n",
    "            if curr_val < prev_val-major_depth_inversion and curr_idx not in depth_inversion_errors_major:\n",
    "                depth_inversion_errors_major.append(curr_idx)\n",
    "print(f'Identified rows with minor depth inversion errors, {len(set(depth_inversion_errors)) - len(set(depth_inversion_errors_major))} total')\n",
    "print(f'Identified rows with major depth inversion errors, {len(set(depth_inversion_errors_major))} total')\n",
    "\n",
    "#move cols around\n",
    "reference_cols = [c for c in big_table_cleaned.columns if \"reference\" in c]\n",
    "code_cols = [c for c in big_table_cleaned.columns if \"code\" in c]\n",
    "other_cols = [c for c in big_table_cleaned.columns if \"reference\" not in c and \"code\" not in c]\n",
    "big_table_cleaned = big_table_cleaned[other_cols + reference_cols + code_cols]\n",
    "\n",
    "rename_map = {}\n",
    "for suffix, cols in core_groups.items():\n",
    "    for col in cols:\n",
    "        rename_map[col] = suffix  # rename to suffix only\n",
    "big_table_cleaned.rename(columns=rename_map, inplace=True)\n",
    "print('Renamed all columns to their suffix')\n",
    "\n",
    "index_v = True\n",
    "min_cols_per = {}\n",
    "\n",
    "min_cols_export = 0\n",
    "excel_path = f'{output_dir}/{project}_full.xlsx'\n",
    "big_table_cleaned = big_table_cleaned[big_table_cleaned.notna().sum(axis=1) >= min_cols_export]\n",
    "big_table_cleaned.to_excel(excel_path, index=index_v)\n",
    "min_cols_per[excel_path] = min_cols_export\n",
    "print(f'Exported cleaned table to excel at {excel_path}')\n",
    "\n",
    "min_pairs_export = 2\n",
    "excel_path = f'{output_dir}/{project}_{min_pairs_export}plus.xlsx'\n",
    "min_cols_export = ((min_pairs_export-1) * 4) + 1 # y = x - 1 (so its minimum cols above the max of 1 less pair), times 4 cols per pair (because of code and ref), plus 1 to be above\n",
    "filtered_big_table = big_table_cleaned[big_table_cleaned.notna().sum(axis=1) >= min_cols_export]\n",
    "filtered_big_table.to_excel(excel_path, index=index_v)\n",
    "min_cols_per[excel_path] = min_pairs_export\n",
    "print(f'Exported filtered table to excel at {excel_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded workbook table_out//Antarctic_full.xlsx for styling\n",
      "Added error corrections for Antarctic_full.xlsx\n",
      "Styled and saved workbook at table_out//Antarctic_full.xlsx\n",
      "Loaded workbook table_out//Antarctic_2plus.xlsx for styling\n",
      "Added error corrections for Antarctic_2plus.xlsx\n",
      "Styled and saved workbook at table_out//Antarctic_2plus.xlsx\n"
     ]
    }
   ],
   "source": [
    "excel_paths = list(min_cols_per.keys())\n",
    "\n",
    "#thick side\n",
    "thick_side = Side(border_style=\"thick\", color=\"000000\") \n",
    "thick_left_border = Border(left=thick_side)\n",
    "medium_side = Side(border_style=\"medium\", color=\"000000\")\n",
    "bottom_medium_border = Border(bottom=medium_side)\n",
    "medium_left_border = Border(left=medium_side)\n",
    "medium_right_border = Border(right=medium_side)\n",
    "medium_border = Border(top=medium_side, left=medium_side, right=medium_side, bottom=medium_side)\n",
    "light_side = Side(border_style=\"thin\", color=\"000000\")\n",
    "light_left_border = Border(left=light_side)\n",
    "\n",
    "for excel_path in excel_paths:\n",
    "    wb = load_workbook(excel_path)\n",
    "    ws = wb.active\n",
    "    print(f\"Loaded workbook {excel_path} for styling\")\n",
    "\n",
    "    #create second page for legend/stats\n",
    "    legend_sheet = wb.create_sheet(title=\"Legend, Stats, and References\")\n",
    "    legend_sheet[\"A1\"] = \"Legend and Stats\"\n",
    "    legend_sheet[\"A1\"].font = Font(size=14, bold=True)\n",
    "    legend_sheet[\"A1\"].alignment = Alignment(horizontal=\"center\")\n",
    "    legend_sheet.merge_cells('A1:D1')  # Merge first row for title\n",
    "    for col_idx in range(1, 5):\n",
    "        legend_sheet.cell(row=1, column=col_idx).border = medium_border\n",
    "    legend_sheet.freeze_panes = \"A2\"\n",
    "\n",
    "    #### Legend sheet reference move\n",
    "\n",
    "    # Find all columns with 'reference' in header (case-insensitive)\n",
    "    reference_cols = [i+1 for i, cell in enumerate(ws[1]) if cell.value and \"reference\" in str(cell.value).lower()]\n",
    "\n",
    "    if reference_cols:\n",
    "        # Copy index column header and data (assumes index col is 1)\n",
    "        legend_sheet.cell(row=1, column=7).value = \"Index\"\n",
    "        legend_sheet.cell(row=1, column=7).font = Font(bold=True)\n",
    "        legend_sheet.cell(row=1, column=7).alignment = Alignment(horizontal=\"center\", vertical=\"center\")\n",
    "        legend_sheet.cell(row=1, column=7).border = medium_border\n",
    "\n",
    "        for row_idx in range(2, ws.max_row + 1): #index copy\n",
    "            legend_sheet.cell(row=row_idx, column=7).value = ws.cell(row=row_idx, column=1).value\n",
    "            legend_sheet.cell(row=row_idx, column=7).alignment = Alignment(horizontal=\"center\", vertical=\"center\")\n",
    "            legend_sheet.cell(row=row_idx, column=7).border = medium_border\n",
    "            legend_sheet.cell(row=row_idx, column=7).fill = PatternFill(start_color=\"FFFFBA\", end_color=\"FFFFBA\", fill_type='solid')  # light yellow fill\n",
    "        # Copy each reference column header and data starting from column 7 (F=6, so G=7 etc)\n",
    "        #rename reference headers\n",
    "        for offset, col_idx in enumerate(reference_cols, start=8):\n",
    "            original_header = ws.cell(row=1, column=col_idx).value\n",
    "            # Split by '_' and keep the first part\n",
    "            new_header = original_header.split('_')[0] if original_header else \"\"\n",
    "            new_header = new_header + ' Ref'\n",
    "            legend_sheet.cell(row=1, column=offset).value = new_header\n",
    "            legend_sheet.cell(row=1, column=offset).alignment = Alignment(horizontal=\"center\", vertical=\"center\")\n",
    "            legend_sheet.cell(row=1, column=offset).font = Font(bold=True)\n",
    "            legend_sheet.cell(row=1, column=offset).border = bottom_medium_border\n",
    "            \n",
    "            offset_letter = get_column_letter(offset)\n",
    "            legend_sheet.column_dimensions[offset_letter].width = 25\n",
    "            for row_idx in range(2, ws.max_row + 1): #value copy\n",
    "                legend_sheet.cell(row=row_idx, column=offset).value = ws.cell(row=row_idx, column=col_idx).value\n",
    "                legend_sheet.cell(row=row_idx, column=offset).border = light_left_border\n",
    "        # Remove reference columns from main sheet, delete from right to left\n",
    "        for col_idx in sorted(reference_cols, reverse=True):\n",
    "            ws.delete_cols(col_idx)\n",
    "\n",
    "    code_cols = [i+1 for i, cell in enumerate(ws[1]) if cell.value and \"code\" in str(cell.value).lower()]\n",
    "\n",
    "    if code_cols:\n",
    "        curr_max_cols = legend_sheet.max_column\n",
    "        # Copy index column header and data (assumes index col is 1)\n",
    "        legend_sheet.cell(row=1, column=curr_max_cols+2).value = \"Index\"\n",
    "        legend_sheet.cell(row=1, column=curr_max_cols+2).font = Font(bold=True)\n",
    "        legend_sheet.cell(row=1, column=curr_max_cols+2).alignment = Alignment(horizontal=\"center\", vertical=\"center\")\n",
    "        legend_sheet.cell(row=1, column=curr_max_cols+2).border = medium_border\n",
    "\n",
    "        for row_idx in range(2, ws.max_row + 1): #index copy\n",
    "            legend_sheet.cell(row=row_idx, column=curr_max_cols+2).value = ws.cell(row=row_idx, column=1).value\n",
    "            legend_sheet.cell(row=row_idx, column=curr_max_cols+2).alignment = Alignment(horizontal=\"center\", vertical=\"center\")\n",
    "            legend_sheet.cell(row=row_idx, column=curr_max_cols+2).border = medium_border\n",
    "            legend_sheet.cell(row=row_idx, column=curr_max_cols+2).fill = PatternFill(start_color=\"FFFFBA\", end_color=\"FFFFBA\", fill_type='solid')  # light yellow fill\n",
    "        # Copy each reference column header and data starting from column 7 (F=6, so G=7 etc)\n",
    "        #rename reference headers\n",
    "        for offset, col_idx in enumerate(code_cols, start=curr_max_cols+3):\n",
    "            original_header = ws.cell(row=1, column=col_idx).value\n",
    "            # Split by '_' and keep the first part\n",
    "            new_header = original_header.split('_')[0] if original_header else \"\"\n",
    "            new_header = new_header + ' Code'\n",
    "            legend_sheet.cell(row=1, column=offset).value = new_header\n",
    "            legend_sheet.cell(row=1, column=offset).alignment = Alignment(horizontal=\"center\", vertical=\"center\")\n",
    "            legend_sheet.cell(row=1, column=offset).font = Font(bold=True)\n",
    "            legend_sheet.cell(row=1, column=offset).border = bottom_medium_border\n",
    "            \n",
    "            offset_letter = get_column_letter(offset)\n",
    "            legend_sheet.column_dimensions[offset_letter].width = 25\n",
    "            for row_idx in range(2, ws.max_row + 1): #value copy\n",
    "                legend_sheet.cell(row=row_idx, column=offset).value = ws.cell(row=row_idx, column=col_idx).value\n",
    "                legend_sheet.cell(row=row_idx, column=offset).border = light_left_border\n",
    "        # Remove reference columns from main sheet, delete from right to left\n",
    "        for col_idx in sorted(code_cols, reverse=True):\n",
    "            ws.delete_cols(col_idx)\n",
    "\n",
    "    ####### Legend Dicts\n",
    "\n",
    "    # Add legend entries\n",
    "    legend = {\n",
    "        \"ffd966\": \"Rows flagged with values differing by > 0.1 but all less than < 0.25\",\n",
    "        \"e06666\": \"Rows flagged with values maximum differing by > 0.25\",\n",
    "    }\n",
    "\n",
    "    legend_row = 3\n",
    "    legend_sheet[f\"A{legend_row}\"] = \"Legend\"\n",
    "    legend_sheet[f\"A{legend_row}\"].font = Font(bold=True)\n",
    "\n",
    "    legend_row +=1\n",
    "    for key, desc in legend.items():\n",
    "        cell = legend_sheet[f\"A{legend_row}\"]\n",
    "        cell.fill = PatternFill(start_color=key, end_color=key, fill_type='solid')  # Set fill style\n",
    "        legend_sheet[f\"B{legend_row}\"] = desc\n",
    "        legend_row += 1\n",
    "\n",
    "    #count how many error on this sheet\n",
    "    num_minor = 0\n",
    "    num_major = 0\n",
    "    for row_idx in range(2, ws.max_row + 1):\n",
    "            cell = ws.cell(row=row_idx, column=1)\n",
    "            if cell.value in within_row_errors:\n",
    "                num_minor += 1\n",
    "            if cell.value in within_row_big_errors:\n",
    "                num_major += 1\n",
    "\n",
    "    # Add some stats\n",
    "    stats = {\n",
    "        \"Total Rows\": f\"{ws.max_row - 1}\",  # assuming ws is your main sheet, -1 for header\n",
    "        \"Total Rows with Errors\": f\"{num_minor} ({num_minor / (ws.max_row - 1) * 100:.2f}%)\",\n",
    "        \"Total Minor Errors (excl. Major)\": f\"{num_minor - num_major} ({(num_minor - num_major) / (ws.max_row - 1) * 100:.2f}%)\",\n",
    "        \"Total Major Errors\": f\"{num_major} ({num_major / (ws.max_row - 1) * 100:.2f}%)\",\n",
    "        \"Minimum Columns per Row\": f\"{min_cols_per[excel_path]}\",\n",
    "        \"Minimum Error Margin\": f\"{error_margin}\",\n",
    "        \"Major Error Margin\": f\"{big_error_margin}\",\n",
    "        \"Original Age Chronology\": f\"{base_core_age}\"\n",
    "    }\n",
    "\n",
    "    stats_row = legend_row + len(legend)\n",
    "    legend_sheet[f\"A{stats_row}\"] = \"Statistics\"\n",
    "    legend_sheet[f\"A{stats_row}\"].font = Font(bold=True)\n",
    "\n",
    "    stats_row +=1\n",
    "    for stat, val in stats.items():\n",
    "        legend_sheet[f\"A{stats_row}\"] = stat\n",
    "        legend_sheet[f\"B{stats_row}\"] = val\n",
    "        stats_row += 1\n",
    "\n",
    "    #adjust readability\n",
    "    legend_sheet.column_dimensions['A'].width = 25  # wider column A in legend\n",
    "    legend_sheet.column_dimensions['B'].width = 25  # wider column A in legend\n",
    "\n",
    "    ### Styling main sheet ----------\n",
    "\n",
    "    # Load headers\n",
    "    headers = [cell.value for cell in ws[1]]\n",
    "\n",
    "   # error coloring and error column\n",
    "    max_columns = ws.max_column\n",
    "    if index_v:\n",
    "        headers_to_color = headers[1:]\n",
    "        start_col = 2   # Excel column index: 1 = index col, 2 = real col 1\n",
    "        ws[\"A1\"].value = \"Index\"\n",
    "        ws[\"A1\"].font = Font(bold=True)\n",
    "        ws[\"A1\"].alignment = Alignment(horizontal=\"center\", vertical=\"center\")\n",
    "\n",
    "        for row_idx in range(2, ws.max_row + 1):\n",
    "            cell = ws.cell(row=row_idx, column=1) # index col\n",
    "            cell.font = Font(bold=False)\n",
    "            cell.alignment = Alignment(horizontal=\"center\", vertical=\"center\")\n",
    "            if cell.value in within_row_errors: # mark minor errors\n",
    "                cell.fill = PatternFill(start_color=\"ffd966\", end_color=\"ffd966\", fill_type='solid')\n",
    "\n",
    "                error_cell = ws.cell(row=row_idx, column=max_columns + 1)\n",
    "                indices = [i for i, v in enumerate(within_row_errors) if v == cell.value]\n",
    "                cores = [within_row_errors_core[i] for i in indices]\n",
    "                error_cell.value = \", \".join(cores)\n",
    "\n",
    "            if cell.value in within_row_big_errors: # mark major errors\n",
    "                cell.fill = PatternFill(start_color=\"e06666\", end_color=\"e06666\", fill_type='solid')\n",
    "\n",
    "            if cell.value in depth_inversion_errors: # mark depth inversion errors\n",
    "                depth_error_cell = ws.cell(row=row_idx, column=max_columns + 2)\n",
    "                depth_error_cell.fill = PatternFill(start_color=\"d5bc7e\", end_color=\"d5bc7e\", fill_type='solid')\n",
    "                indices = [i for i, v in enumerate(depth_inversion_errors) if v == cell.value] # get all indices for this row\n",
    "                cores = [depth_inversion_errors_core[i] for i in indices] # get all cores for this row\n",
    "                depth_error_cell.value = \", \".join(cores)\n",
    "            \n",
    "            if cell.value in depth_inversion_errors_major: # mark major errors\n",
    "                depth_error_cell = ws.cell(row=row_idx, column=max_columns + 2)\n",
    "                depth_error_cell.fill = PatternFill(start_color=\"d57e7e\", end_color=\"d57e7e\", fill_type='solid')\n",
    "\n",
    "        error_column = get_column_letter(max_columns + 1)\n",
    "        ws.column_dimensions[error_column].width = 25\n",
    "        depth_error_column = get_column_letter(max_columns + 2)\n",
    "        ws.column_dimensions[depth_error_column].width = 25\n",
    "        print(f'Added error corrections for {os.path.basename(excel_path)}')\n",
    "\n",
    "    else:\n",
    "        headers_to_color = headers\n",
    "        start_col = 1\n",
    "\n",
    "    # Define distinct light colors\n",
    "    colors = [\n",
    "        \"FFB3BA\", \"FFDFBA\", \"FFFFBA\", \"BAFFC9\", \"BAE1FF\",\n",
    "        \"D7BAFF\", \"FFC3F7\", \"BAFFD9\", \"FFE0BA\", \"D0BAFF\"\n",
    "    ]\n",
    "\n",
    "    colors_note = [\n",
    "    \"#FFB3BA\", \"#FFDFBA\", \"#FFFFBA\", \"#BAFFC9\", \"#BAE1FF\",\n",
    "    \"#D7BAFF\", \"#FFC3F7\", \"#BAFFD9\", \"#FFE0BA\", \"#D0BAFF\"]\n",
    "\n",
    "    # Assign colors to unique header names\n",
    "    color_map = {}\n",
    "    for col_name in headers_to_color:\n",
    "        if col_name not in color_map:\n",
    "            color_map[col_name] = colors[len(color_map) % len(colors)]\n",
    "\n",
    "    # Apply fill color to each column\n",
    "    col_idx = start_col\n",
    "    for col_name in headers_to_color:\n",
    "\n",
    "        fill = PatternFill(\n",
    "            start_color=color_map[col_name],\n",
    "            end_color=color_map[col_name],\n",
    "            fill_type='solid'\n",
    "        )\n",
    "\n",
    "        # Color header\n",
    "        ws.cell(row=1, column=col_idx).fill = fill\n",
    "\n",
    "        # Color all data rows\n",
    "        for row_idx in range(2, ws.max_row + 1):\n",
    "            ws.cell(row=row_idx, column=col_idx).fill = fill\n",
    "\n",
    "        col_idx += 1\n",
    "\n",
    "    # Loop through columns starting at column 3 (Excel index), applying thick border every 2 columns \n",
    "    for col_idx in range(2, max_columns + 2, 2): # 3, 5, 7, 9 ... \n",
    "        for row_idx in range(1, ws.max_row + 1):\n",
    "            ws.cell(row=row_idx, column=col_idx).border = thick_left_border\n",
    "\n",
    "    # Medium thick on bottom of row 1\n",
    "    medium_border_int = 1 if index_v else 0\n",
    "    for col_idx in range(1, ws.max_column + medium_border_int):\n",
    "        ws.cell(row=1, column=col_idx).border = bottom_medium_border\n",
    "\n",
    "    #freeze top row\n",
    "    ws.freeze_panes = \"A2\"\n",
    "\n",
    "    #add error column header\n",
    "    if index_v:\n",
    "        error_col_cell = ws.cell(row=1, column=max_columns + 1)\n",
    "        error_col_cell.value = \"Within Row Errors\"\n",
    "        error_col_cell.font = Font(bold=True)\n",
    "        error_col_cell.alignment = Alignment(horizontal=\"center\", vertical=\"center\")\n",
    "\n",
    "        depth_error_col_cell = ws.cell(row=1, column=max_columns + 2)\n",
    "        depth_error_col_cell.value = \"Depth Inversion Errors\"\n",
    "        depth_error_col_cell.font = Font(bold=True)\n",
    "        depth_error_col_cell.alignment = Alignment(horizontal=\"center\", vertical=\"center\")\n",
    "\n",
    "    #rename sheet\n",
    "    ws.title = \"Ice Core Depth Comparison\"\n",
    "\n",
    "    # Save workbook\n",
    "    wb.save(excel_path)\n",
    "    print(f\"Styled and saved workbook at {excel_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bicc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
