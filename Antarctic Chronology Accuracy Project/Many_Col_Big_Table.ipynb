{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import os\n",
    "import yaml\n",
    "import itertools\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from openpyxl.styles import PatternFill\n",
    "from openpyxl import load_workbook\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "from openpyxl.styles import Border, Side\n",
    "from openpyxl.styles import Alignment, Font"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['EDC-WDC',\n",
       " 'EDC-EDML',\n",
       " 'EDC-DF',\n",
       " 'EDC-TALDICE',\n",
       " 'WDC-EDML',\n",
       " 'WDC-DF',\n",
       " 'WDC-TALDICE',\n",
       " 'EDML-DF',\n",
       " 'EDML-TALDICE',\n",
       " 'DF-TALDICE']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cores list of sites for AA\n",
    "\n",
    "project = 'AA_14Cols'\n",
    "output_dir = '/Users/quinnmackay/Desktop/table_out'\n",
    "\n",
    "# get all link combos\n",
    "with open(f'/Users/quinnmackay/Documents/GitHub/BICC/Antarctic Chronology Accuracy Project/{project}/parameters.yml') as f:\n",
    "    data = yaml.safe_load(f)\n",
    "list_sites = data[\"list_sites\"]\n",
    "pairs = [f\"{a}-{b}\" for a, b in itertools.combinations(list_sites, 2)]\n",
    "\n",
    "error_margin = 0.1\n",
    "big_error_margin = 0.25\n",
    "pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed pair EDC-WDC, total points after merging: 644, (897 original total rows)\n",
      "Processed pair EDC-EDML, total points after merging: 489, (827 original total rows)\n",
      "Processed pair EDC-DF, total points after merging: 1451, (1605 original total rows)\n",
      "Processed pair EDC-TALDICE, total points after merging: 203, (246 original total rows)\n",
      "Processed pair WDC-EDML, total points after merging: 982, (1315 original total rows)\n",
      "Processed pair WDC-DF, total points after merging: 626, (1078 original total rows)\n",
      "Processed pair WDC-TALDICE, total points after merging: 664, (1219 original total rows)\n",
      "Processed pair EDML-DF, total points after merging: 215, (215 original total rows)\n",
      "Processed pair EDML-TALDICE, total points after merging: 128, (128 original total rows)\n",
      "Processed pair DF-TALDICE, total points after merging: 111, (111 original total rows)\n"
     ]
    }
   ],
   "source": [
    "big_table = pd.DataFrame()\n",
    "\n",
    "for core in list_sites: # loop through each core\n",
    "    for comparison_core in list_sites: # loop through each core other than the initial load\n",
    "        pair = f\"{core}-{comparison_core}\"\n",
    "        if core != comparison_core and pair in pairs: # make sure not the same core and we skip non-existent linkages\n",
    "            pair_dir = Path(f'/Users/quinnmackay/Documents/GitHub/BICC/Antarctic Chronology Accuracy Project/{project}/{pair}')\n",
    "\n",
    "            # Check: directory exists AND contains at least one .txt file\n",
    "            txt_files = list(pair_dir.glob(\"*.txt\"))\n",
    "            if not pair_dir.is_dir() or not txt_files:\n",
    "                continue\n",
    "\n",
    "            dfs=[] #load all text files into one\n",
    "            for txt in txt_files:\n",
    "                df = pd.read_csv(txt, sep=\"\\t\", comment=\"#\")\n",
    "                dfs.append(df)\n",
    "    \n",
    "            num_files = len(dfs)\n",
    "            load_data = pd.concat(dfs, ignore_index=True)\n",
    "            original_rows = len(load_data)\n",
    "\n",
    "            drop_rows = []\n",
    "            drop_rows_merge = set()\n",
    "            new_merged_rows = []\n",
    "            for idx, row in load_data.iterrows():\n",
    "\n",
    "                mask1 = abs(row['depth1'] - load_data['depth1']) <= error_margin\n",
    "                mask1[idx] = False\n",
    "                mask2 = abs(row['depth2'] - load_data['depth2']) <= error_margin \n",
    "                mask2[idx] = False\n",
    "\n",
    "                close_points = load_data[mask1 & mask2]\n",
    "                num_close = len(close_points)\n",
    "                close_idxs = load_data.index[mask1 & mask2]\n",
    "\n",
    "                if num_close > 0:\n",
    "                    refs = [load_data.at[idx, 'reference']] + [load_data.at[i, 'reference'] for i in close_idxs] #adjoin references\n",
    "                    merged_ref = \"; \".join(str(r) for r in refs if pd.notna(r))\n",
    "\n",
    "                    depth1_vals = [load_data.at[idx, 'depth1']] + [load_data.at[i, 'depth1'] for i in close_idxs]\n",
    "                    merged_depth1 = np.mean(depth1_vals)\n",
    "\n",
    "                    depth2_vals = [load_data.at[idx, 'depth2']] + [load_data.at[i, 'depth2'] for i in close_idxs]\n",
    "                    merged_depth2 = np.mean(depth2_vals)\n",
    "\n",
    "                    new_merged_rows.append({'reference': merged_ref, 'depth1': merged_depth1, 'depth2': merged_depth2}) #create new merged row\n",
    "\n",
    "                    drop_rows_merge.add(idx)\n",
    "                    for i in close_idxs:\n",
    "                        drop_rows.append(i)\n",
    "                        if drop_rows.count(i) >= num_files:\n",
    "                            print(f'WARNING: Row {load_data.at[i, 'depth1']} | {load_data.at[i, 'depth2']} for {pair}. Reference {load_data.at[i, 'reference']}.')\n",
    "                            print(f'Called by row {load_data.at[idx, 'depth1']} | {load_data.at[idx, 'depth2']} from reference {load_data.at[idx, 'reference']}.')\n",
    "\n",
    "            # drop duplicate rows\n",
    "            drop_rows = set(drop_rows).union(drop_rows_merge)\n",
    "            load_data = load_data.drop(index=drop_rows).reset_index(drop=True)\n",
    "            # add merged rows\n",
    "            merged_df = pd.DataFrame(new_merged_rows)\n",
    "            load_data = pd.concat([load_data, merged_df], ignore_index=True)\n",
    "            load_data.drop_duplicates(subset=['depth1', 'depth2'], inplace=True)\n",
    "            load_data = load_data.reset_index(drop=True)\n",
    "\n",
    "            load_data = load_data.sort_values(by=['depth1']).reset_index(drop=True)\n",
    "\n",
    "            # rename to create unique columns for this pair\n",
    "            load_data = load_data.rename(columns={\n",
    "                'depth1': f\"{pair}_{core}\",\n",
    "                'depth2': f\"{pair}_{comparison_core}\",\n",
    "                'reference': f\"{pair}_reference\"\n",
    "            })\n",
    "\n",
    "            print(f\"Processed pair {pair}, total points after merging: {len(load_data)}, ({original_rows} original total rows)\")\n",
    "            # append rows (block)\n",
    "            big_table = pd.concat([big_table, load_data],\n",
    "                                  axis=0,\n",
    "                                  ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total updates made: 9470 (+9470)\n",
      "total updates made: 10061 (+591)\n",
      "total updates made: 10064 (+3)\n",
      "total updates made: 10064 (+0)\n",
      "Reduced table by 56.98% due to duplicates\n",
      "Identified rows with small within-row errors, 3 total\n",
      "Identified rows with big within-row errors, 56 total\n",
      "Renamed all columns to their suffix\n",
      "Exported cleaned table to excel at /Users/quinnmackay/Desktop/table_out/GL_20Cols_full.xlsx\n",
      "Exported 3+ filtered table to excel at /Users/quinnmackay/Desktop/table_out/GL_20Cols_3plus.xlsx\n"
     ]
    }
   ],
   "source": [
    "core_groups = defaultdict(list)\n",
    "matching_groups = defaultdict(list)\n",
    "\n",
    "for col in big_table.columns:\n",
    "    suffix = col.split(\"_\")[-1]\n",
    "    core_groups[suffix].append(col) #group cols by suffix\n",
    "    \n",
    "    match = col.split(\"_\")[0]\n",
    "    core1 = match.split(\"-\")[0]\n",
    "    core2 = match.split(\"-\")[1]\n",
    "\n",
    "    if core1 == suffix:\n",
    "        matching_core = core2\n",
    "    elif core2 == suffix:\n",
    "        matching_core = core1\n",
    "\n",
    "    matching_groups[suffix].append(f\"{match}_{matching_core}\")\n",
    "\n",
    "update_check = 0\n",
    "refresh = 1\n",
    "while refresh > 0:\n",
    "    refresh = 0\n",
    "    for core, assoc_cols in core_groups.items():\n",
    "        matching_cols = matching_groups[core]\n",
    "\n",
    "        for col, match_col in zip(assoc_cols, matching_cols):\n",
    "            for col_check in assoc_cols:\n",
    "                if col == col_check:\n",
    "                    continue\n",
    "\n",
    "                col_updates = {}\n",
    "                match_updates = {}\n",
    "\n",
    "                for index, value in big_table[col].items():\n",
    "                    diff = (big_table[col_check] - value).abs()\n",
    "                    matching_indices = diff[diff <= error_margin].index\n",
    "\n",
    "                    for match_idx in matching_indices:\n",
    "                        col_updates[match_idx] = big_table[col].at[index]\n",
    "                        match_updates[match_idx] = big_table[match_col].at[index]\n",
    "            \n",
    "                for match_idx, new_val in col_updates.items():\n",
    "                    if pd.isna(big_table.at[match_idx, col]):\n",
    "                        big_table.at[match_idx, col] = new_val\n",
    "                        update_check+=1\n",
    "                        refresh+=1\n",
    "                for match_idx, new_val in match_updates.items():\n",
    "                    if pd.isna(big_table.at[match_idx, match_col]):\n",
    "                        big_table.at[match_idx, match_col] = new_val\n",
    "            \n",
    "    print(f'total updates made: {update_check} (+{refresh})')\n",
    "\n",
    "duplicates_mask = big_table.duplicated(keep='first')\n",
    "num_dupe = (len(duplicates_mask[duplicates_mask == True]))\n",
    "big_table_cleaned = big_table.drop_duplicates(keep='first').reset_index(drop=True)\n",
    "print(f'Reduced table by {num_dupe/len(big_table)*100:.2f}% due to duplicates')\n",
    "\n",
    "# Do evaluation for errors\n",
    "within_row_errors = []\n",
    "within_row_errors_core = []\n",
    "within_row_big_errors = []\n",
    "within_row_big_errors_core = []\n",
    "for index, row in big_table_cleaned.iterrows(): # Iterate over every row in the table\n",
    "    for core, columns in core_groups.items(): # For each core and its associated list of column names\n",
    "        values = []\n",
    "        for col in columns: # Collect the values for this core on this row\n",
    "            values.append(row[col])\n",
    "        values = [v for v in values if not pd.isna(v)] # Remove NaN values so they don't interfere with comparison\n",
    "        if len(values) >= 2:\n",
    "            diff = abs(max(values) - min(values))\n",
    "            if diff >= error_margin and index not in within_row_errors:\n",
    "                within_row_errors.append(index)\n",
    "                within_row_errors_core.append(core)\n",
    "                #print(f\"Row {index} core {core} values: {values} diff={diff}\")\n",
    "            if diff >= big_error_margin and index not in within_row_big_errors:\n",
    "                within_row_big_errors.append(index)\n",
    "print(f'Identified rows with small within-row errors, {len(within_row_errors) - len(within_row_big_errors)} total')\n",
    "print(f'Identified rows with big within-row errors, {len(within_row_big_errors)} total')\n",
    "\n",
    "rename_map = {}\n",
    "for suffix, cols in core_groups.items():\n",
    "    for col in cols:\n",
    "        rename_map[col] = suffix  # rename to suffix only\n",
    "big_table_cleaned.rename(columns=rename_map, inplace=True)\n",
    "print('Renamed all columns to their suffix')\n",
    "\n",
    "index_v = True\n",
    "min_cols_per = {}\n",
    "\n",
    "min_cols_export = 0\n",
    "excel_path = f'{output_dir}/{project}_full.xlsx'\n",
    "big_table_cleaned = big_table_cleaned[big_table_cleaned.notna().sum(axis=1) >= min_cols_export]\n",
    "big_table_cleaned.to_excel(excel_path, index=index_v)\n",
    "min_cols_per[excel_path] = min_cols_export\n",
    "print(f'Exported cleaned table to excel at {excel_path}')\n",
    "\n",
    "min_cols_export = 3\n",
    "excel_path = f'{output_dir}/{project}_{min_cols_export}plus.xlsx'\n",
    "filtered_big_table = big_table_cleaned[big_table_cleaned.notna().sum(axis=1) >= min_cols_export]\n",
    "filtered_big_table.to_excel(excel_path, index=index_v)\n",
    "min_cols_per[excel_path] = min_cols_export\n",
    "print(f'Exported 3+ filtered table to excel at {excel_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded workbook /Users/quinnmackay/Desktop/table_out/GL_20Cols_full.xlsx for styling\n",
      "Added error corrections for GL_20Cols_full.xlsx\n",
      "Styled and saved workbook at /Users/quinnmackay/Desktop/table_out/GL_20Cols_full.xlsx\n",
      "Loaded workbook /Users/quinnmackay/Desktop/table_out/GL_20Cols_3plus.xlsx for styling\n",
      "Added error corrections for GL_20Cols_3plus.xlsx\n",
      "Styled and saved workbook at /Users/quinnmackay/Desktop/table_out/GL_20Cols_3plus.xlsx\n"
     ]
    }
   ],
   "source": [
    "excel_paths = list(min_cols_per.keys())\n",
    "\n",
    "for excel_path in excel_paths:\n",
    "    wb = load_workbook(excel_path)\n",
    "    ws = wb.active\n",
    "    print(f\"Loaded workbook {excel_path} for styling\")\n",
    "\n",
    "    # Load headers\n",
    "    headers = [cell.value for cell in ws[1]]\n",
    "\n",
    "   # error coloring and error column\n",
    "    max_columns = ws.max_column\n",
    "    if index_v:\n",
    "        headers_to_color = headers[1:]\n",
    "        start_col = 2   # Excel column index: 1 = index col, 2 = real col 1\n",
    "        ws[\"A1\"].value = \"Index\"\n",
    "        ws[\"A1\"].font = Font(bold=True)\n",
    "        ws[\"A1\"].alignment = Alignment(horizontal=\"center\", vertical=\"center\")\n",
    "\n",
    "        for row_idx in range(2, ws.max_row + 1):\n",
    "            cell = ws.cell(row=row_idx, column=1)\n",
    "            cell.font = Font(bold=False)\n",
    "            cell.alignment = Alignment(horizontal=\"center\", vertical=\"center\")\n",
    "            if cell.value in within_row_errors:\n",
    "                cell.fill = PatternFill(start_color=\"ffd966\", end_color=\"ffd966\", fill_type='solid')\n",
    "\n",
    "                within_idx = within_row_errors.index(cell.value)\n",
    "                error_cell = ws.cell(row=row_idx, column=max_columns + 1)\n",
    "                existing_str = str(error_cell.value) if error_cell.value is not None else \"\"\n",
    "                if existing_str == \"\":\n",
    "                    error_cell.value = within_row_errors_core[within_idx]\n",
    "                else:\n",
    "                    error_cell.value = existing_str + \",\" + within_row_errors_core[within_idx]\n",
    "            if cell.value in within_row_big_errors:\n",
    "                cell.fill = PatternFill(start_color=\"e06666\", end_color=\"e06666\", fill_type='solid')\n",
    "        print(f'Added error corrections for {os.path.basename(excel_path)}')\n",
    "\n",
    "    else:\n",
    "        headers_to_color = headers\n",
    "        start_col = 1\n",
    "\n",
    "    # Define distinct light colors\n",
    "    colors = [\n",
    "        \"FFB3BA\", \"FFDFBA\", \"FFFFBA\", \"BAFFC9\", \"BAE1FF\",\n",
    "        \"D7BAFF\", \"FFC3F7\", \"BAFFD9\", \"FFE0BA\", \"D0BAFF\"\n",
    "    ]\n",
    "\n",
    "    colors_note = [\n",
    "    \"#FFB3BA\", \"#FFDFBA\", \"#FFFFBA\", \"#BAFFC9\", \"#BAE1FF\",\n",
    "    \"#D7BAFF\", \"#FFC3F7\", \"#BAFFD9\", \"#FFE0BA\", \"#D0BAFF\"]\n",
    "\n",
    "    # Assign colors to unique header names\n",
    "    color_map = {}\n",
    "    for col_name in headers_to_color:\n",
    "        if col_name not in color_map:\n",
    "            color_map[col_name] = colors[len(color_map) % len(colors)]\n",
    "\n",
    "    # Apply fill color to each column\n",
    "    col_idx = start_col\n",
    "    for col_name in headers_to_color:\n",
    "\n",
    "        fill = PatternFill(\n",
    "            start_color=color_map[col_name],\n",
    "            end_color=color_map[col_name],\n",
    "            fill_type='solid'\n",
    "        )\n",
    "\n",
    "        # Color header\n",
    "        ws.cell(row=1, column=col_idx).fill = fill\n",
    "\n",
    "        # Color all data rows\n",
    "        for row_idx in range(2, ws.max_row + 1):\n",
    "            ws.cell(row=row_idx, column=col_idx).fill = fill\n",
    "\n",
    "        col_idx += 1\n",
    "\n",
    "    # #Define a thick border on the left side of a column \n",
    "    thick_side = Side(border_style=\"thick\", color=\"000000\") \n",
    "    thick_left_border = Border(left=thick_side)\n",
    "    # Loop through columns starting at column 3 (Excel index), applying thick border every 2 columns \n",
    "    for col_idx in range(2, ws.max_column + 2, 2): # 3, 5, 7, 9 ... \n",
    "        for row_idx in range(1, ws.max_row + 1):\n",
    "            ws.cell(row=row_idx, column=col_idx).border = thick_left_border\n",
    "\n",
    "    # Medium thick on bottom of row 1\n",
    "    medium_border = 1 if index_v else 0\n",
    "    medium_side = Side(border_style=\"medium\", color=\"000000\")\n",
    "    bottom_border = Border(bottom=medium_side)\n",
    "    for col_idx in range(1, ws.max_column + medium_border):\n",
    "        ws.cell(row=1, column=col_idx).border = bottom_border\n",
    "\n",
    "    #freeze top row\n",
    "    ws.freeze_panes = \"A2\"\n",
    "\n",
    "    #add error column header\n",
    "    if index_v:\n",
    "        error_col_cell = ws.cell(row=1, column=max_columns + 1)\n",
    "        error_col_cell.value = \"Error Cores\"\n",
    "        error_col_cell.font = Font(bold=True)\n",
    "        error_col_cell.alignment = Alignment(horizontal=\"center\", vertical=\"center\")\n",
    "\n",
    "    #rename sheet\n",
    "    ws.title = \"Ice Core Depth Comparison\"\n",
    "\n",
    "    #create second page for legend/stats\n",
    "    legend_sheet = wb.create_sheet(title=\"Legend & Stats\")\n",
    "    legend_sheet[\"A1\"] = \"Legend and Stats\"\n",
    "    legend_sheet[\"A1\"].font = Font(size=14, bold=True)\n",
    "    legend_sheet[\"A1\"].alignment = Alignment(horizontal=\"center\")\n",
    "    legend_sheet.merge_cells('A1:D1')  # Merge first row for title\n",
    "\n",
    "    # Add legend entries (example)\n",
    "    legend = {\n",
    "        \"ffd966\": \"Rows flagged with values differing by > 0.1 but all less than < 0.25\",\n",
    "        \"e06666\": \"Rows flagged with values maximum differing by > 0.25\",\n",
    "    }\n",
    "\n",
    "    legend_row = 3\n",
    "    legend_sheet[f\"A{legend_row}\"] = \"Legend\"\n",
    "    legend_sheet[f\"A{legend_row}\"].font = Font(bold=True)\n",
    "\n",
    "    legend_row +=1\n",
    "    for key, desc in legend.items():\n",
    "        cell = legend_sheet[f\"A{legend_row}\"]\n",
    "        cell.fill = PatternFill(start_color=key, end_color=key, fill_type='solid')  # Set fill style\n",
    "        legend_sheet[f\"B{legend_row}\"] = desc\n",
    "        legend_row += 1\n",
    "\n",
    "    # Add some stats (example)\n",
    "    stats = {\n",
    "        \"Total Rows\": ws.max_row - 1,  # assuming ws is your main sheet, -1 for header\n",
    "        \"Total Rows with Errors\": f\"{len(within_row_errors)} ({len(within_row_errors) / (ws.max_row - 1) * 100:.2f}%)\",\n",
    "        \"Total Minor Errors (excl. Major)\": f\"{len(within_row_errors) - len(within_row_big_errors)} ({(len(within_row_errors) - len(within_row_big_errors)) / (ws.max_row - 1) * 100:.2f}%)\",\n",
    "        \"Total Major Errors\": f\"{len(within_row_big_errors)} ({len(within_row_big_errors) / (ws.max_row - 1) * 100:.2f}%)\",\n",
    "        \"Minimum Columns per Row\": f\"{min_cols_per[excel_path]}\",\n",
    "    }\n",
    "\n",
    "    stats_row = legend_row + len(legend)\n",
    "    legend_sheet[f\"A{stats_row}\"] = \"Statistics\"\n",
    "    legend_sheet[f\"A{stats_row}\"].font = Font(bold=True)\n",
    "\n",
    "    stats_row +=1\n",
    "    for stat, val in stats.items():\n",
    "        legend_sheet[f\"A{stats_row}\"] = stat\n",
    "        legend_sheet[f\"B{stats_row}\"] = val\n",
    "        stats_row += 1\n",
    "\n",
    "    #adjust readability\n",
    "    legend_sheet.column_dimensions['A'].width = 25  # wider column A in legend\n",
    "\n",
    "    # Save workbook\n",
    "    wb.save(excel_path)\n",
    "    print(f\"Styled and saved workbook at {excel_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bicc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
