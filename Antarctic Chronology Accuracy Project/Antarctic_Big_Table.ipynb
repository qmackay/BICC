{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import os\n",
    "import yaml\n",
    "import itertools\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from openpyxl.styles import PatternFill\n",
    "from openpyxl import load_workbook\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "from openpyxl.styles import Border, Side\n",
    "from openpyxl.styles import Alignment, Font\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['EDC-WDC',\n",
       " 'EDC-EDML',\n",
       " 'EDC-DF',\n",
       " 'EDC-TALDICE',\n",
       " 'WDC-EDML',\n",
       " 'WDC-DF',\n",
       " 'WDC-TALDICE',\n",
       " 'EDML-DF',\n",
       " 'EDML-TALDICE',\n",
       " 'DF-TALDICE']"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cores list of sites for AA\n",
    "\n",
    "list_sites = ['EDC', 'WDC', 'EDML', 'DF', 'TALDICE']\n",
    "project = 'AA_14Cols'\n",
    "\n",
    "# get all link combos\n",
    "pairs = [f\"{a}-{b}\" for a, b in itertools.combinations(list_sites, 2)]\n",
    "\n",
    "error_margin = 0.1\n",
    "big_error_margin = 0.25\n",
    "pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_table = pd.DataFrame()\n",
    "\n",
    "for core in list_sites: # loop through each core\n",
    "    for comparison_core in list_sites: # loop through each core other than the initial load\n",
    "        pair = f\"{core}-{comparison_core}\"\n",
    "        if core != comparison_core and pair in pairs: # make sure not the same core and we skip non-existent linkages\n",
    "            file = Path(f'/Users/quinnmackay/Documents/GitHub/BICC/Antarctic Chronology Accuracy Project/{project}/{pair}/iceice_synchro_horizons.txt')\n",
    "\n",
    "            if not file.exists():\n",
    "                continue\n",
    "\n",
    "            with open(file, 'r') as f:\n",
    "                skip = sum(1 for line in f if line.startswith('#'))\n",
    "\n",
    "            load_data = pd.read_csv(file, sep='\\t', comment='#', skiprows=skip+1, names=[core, comparison_core], usecols=[0,1])\n",
    "\n",
    "            # rename to create unique columns for this pair\n",
    "            load_data = load_data.rename(columns={\n",
    "                core: f\"{pair}_{core}\",\n",
    "                comparison_core: f\"{pair}_{comparison_core}\"\n",
    "            })\n",
    "\n",
    "            # append rows (block)\n",
    "            big_table = pd.concat([big_table, load_data],\n",
    "                                  axis=0,\n",
    "                                  ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total updates made: 9618 (+9618)\n",
      "total updates made: 10061 (+443)\n",
      "total updates made: 10069 (+8)\n",
      "total updates made: 10069 (+0)\n",
      "Reduced table by 45.18% due to duplicates\n",
      "Identified rows with within-row errors, 116 total\n",
      "Identified rows with big within-row errors, 12 total\n",
      "Renamed all columns to their suffix\n",
      "Exported cleaned table to excel at /Users/quinnmackay/Desktop/temp/bicc_aa_14cols_merged.xlsx\n",
      "Exported 3+ filtered table to excel at /Users/quinnmackay/Desktop/temp/bicc_aa_14cols_merged_3plus.xlsx\n"
     ]
    }
   ],
   "source": [
    "core_groups = defaultdict(list)\n",
    "matching_groups = defaultdict(list)\n",
    "\n",
    "for col in big_table.columns:\n",
    "    suffix = col.split(\"_\")[-1]\n",
    "    core_groups[suffix].append(col) #group cols by suffix\n",
    "    \n",
    "    match = col.split(\"_\")[0]\n",
    "    core1 = match.split(\"-\")[0]\n",
    "    core2 = match.split(\"-\")[1]\n",
    "\n",
    "    if core1 == suffix:\n",
    "        matching_core = core2\n",
    "    elif core2 == suffix:\n",
    "        matching_core = core1\n",
    "\n",
    "    matching_groups[suffix].append(f\"{match}_{matching_core}\")\n",
    "\n",
    "update_check = 0\n",
    "refresh = 1\n",
    "while refresh > 0:\n",
    "    refresh = 0\n",
    "    for core, assoc_cols in core_groups.items():\n",
    "        matching_cols = matching_groups[core]\n",
    "\n",
    "        for col, match_col in zip(assoc_cols, matching_cols):\n",
    "            for col_check in assoc_cols:\n",
    "                if col == col_check:\n",
    "                    continue\n",
    "\n",
    "                col_updates = {}\n",
    "                match_updates = {}\n",
    "\n",
    "                for index, value in big_table[col].items():\n",
    "                    diff = (big_table[col_check] - value).abs()\n",
    "                    matching_indices = diff[diff <= error_margin].index\n",
    "\n",
    "                    for match_idx in matching_indices:\n",
    "                        col_updates[match_idx] = big_table[col].at[index]\n",
    "                        match_updates[match_idx] = big_table[match_col].at[index]\n",
    "            \n",
    "                for match_idx, new_val in col_updates.items():\n",
    "                    if pd.isna(big_table.at[match_idx, col]):\n",
    "                        big_table.at[match_idx, col] = new_val\n",
    "                        update_check+=1\n",
    "                        refresh+=1\n",
    "                for match_idx, new_val in match_updates.items():\n",
    "                    if pd.isna(big_table.at[match_idx, match_col]):\n",
    "                        big_table.at[match_idx, match_col] = new_val\n",
    "            \n",
    "    print(f'total updates made: {update_check} (+{refresh})')\n",
    "\n",
    "duplicates_mask = big_table.duplicated(keep='first')\n",
    "num_dupe = (len(duplicates_mask[duplicates_mask == True]))\n",
    "big_table_cleaned = big_table.drop_duplicates(keep='first').reset_index(drop=True)\n",
    "print(f'Reduced table by {num_dupe/len(big_table)*100:.2f}% due to duplicates')\n",
    "\n",
    "# Do evaluation for errors\n",
    "within_row_errors = []\n",
    "within_row_errors_core = []\n",
    "within_row_big_errors = []\n",
    "within_row_big_errors_core = []\n",
    "for index, row in big_table_cleaned.iterrows(): # Iterate over every row in the table\n",
    "    for core, columns in core_groups.items(): # For each core and its associated list of column names\n",
    "        values = []\n",
    "        for col in columns: # Collect the values for this core on this row\n",
    "            values.append(row[col])\n",
    "        values = [v for v in values if not pd.isna(v)] # Remove NaN values so they don't interfere with comparison\n",
    "        if len(values) >= 2:\n",
    "            diff = abs(max(values) - min(values))\n",
    "            if diff >= error_margin and index not in within_row_errors:\n",
    "                within_row_errors.append(index)\n",
    "                within_row_errors_core.append(core)\n",
    "                #print(f\"Row {index} core {core} values: {values} diff={diff}\")\n",
    "            elif diff >= big_error_margin and index not in within_row_big_errors:\n",
    "                within_row_big_errors.append(index)\n",
    "                within_row_big_errors_core.append(core)\n",
    "print(f'Identified rows with within-row errors, {len(within_row_errors)} total')\n",
    "print(f'Identified rows with big within-row errors, {len(within_row_big_errors)} total')\n",
    "\n",
    "rename_map = {}\n",
    "for suffix, cols in core_groups.items():\n",
    "    for col in cols:\n",
    "        rename_map[col] = suffix  # rename to suffix only\n",
    "big_table_cleaned.rename(columns=rename_map, inplace=True)\n",
    "print('Renamed all columns to their suffix')\n",
    "\n",
    "index_v = True\n",
    "min_cols_per = {}\n",
    "\n",
    "min_cols_export = 0\n",
    "excel_path = '/Users/quinnmackay/Desktop/temp/bicc_aa_14cols_merged.xlsx'\n",
    "big_table_cleaned = big_table_cleaned[big_table_cleaned.notna().sum(axis=1) >= min_cols_export]\n",
    "big_table_cleaned.to_excel(excel_path, index=index_v)\n",
    "min_cols_per[excel_path] = min_cols_export\n",
    "print(f'Exported cleaned table to excel at {excel_path}')\n",
    "\n",
    "min_cols_export = 3\n",
    "excel_path = '/Users/quinnmackay/Desktop/temp/bicc_aa_14cols_merged_3plus.xlsx'\n",
    "filtered_big_table = big_table_cleaned[big_table_cleaned.notna().sum(axis=1) >= min_cols_export]\n",
    "filtered_big_table.to_excel(excel_path, index=index_v)\n",
    "min_cols_per[excel_path] = min_cols_export\n",
    "print(f'Exported 3+ filtered table to excel at {excel_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded workbook /Users/quinnmackay/Desktop/temp/bicc_aa_14cols_merged.xlsx for styling\n",
      "Added error corrections for bicc_aa_14cols_merged.xlsx\n",
      "Styled and saved workbook at /Users/quinnmackay/Desktop/temp/bicc_aa_14cols_merged.xlsx\n",
      "Loaded workbook /Users/quinnmackay/Desktop/temp/bicc_aa_14cols_merged_3plus.xlsx for styling\n",
      "Added error corrections for bicc_aa_14cols_merged_3plus.xlsx\n",
      "Styled and saved workbook at /Users/quinnmackay/Desktop/temp/bicc_aa_14cols_merged_3plus.xlsx\n"
     ]
    }
   ],
   "source": [
    "excel_paths = list(min_cols_per.keys())\n",
    "\n",
    "for excel_path in excel_paths:\n",
    "    wb = load_workbook(excel_path)\n",
    "    ws = wb.active\n",
    "    print(f\"Loaded workbook {excel_path} for styling\")\n",
    "\n",
    "    # Load headers\n",
    "    headers = [cell.value for cell in ws[1]]\n",
    "\n",
    "   # error coloring and error column\n",
    "    max_columns = ws.max_column\n",
    "    if index_v:\n",
    "        headers_to_color = headers[1:]\n",
    "        start_col = 2   # Excel column index: 1 = index col, 2 = real col 1\n",
    "        ws[\"A1\"].value = \"Index\"\n",
    "        ws[\"A1\"].font = Font(bold=True)\n",
    "        ws[\"A1\"].alignment = Alignment(horizontal=\"center\", vertical=\"center\")\n",
    "\n",
    "        for row_idx in range(2, ws.max_row + 1):\n",
    "            cell = ws.cell(row=row_idx, column=1)\n",
    "            cell.font = Font(bold=False)\n",
    "            cell.alignment = Alignment(horizontal=\"center\", vertical=\"center\")\n",
    "            if cell.value in within_row_errors:\n",
    "                cell.fill = PatternFill(start_color=\"ffd966\", end_color=\"ffd966\", fill_type='solid')\n",
    "\n",
    "                within_idx = within_row_errors.index(cell.value)\n",
    "                error_cell = ws.cell(row=row_idx, column=max_columns + 1)\n",
    "                existing_str = str(error_cell.value) if error_cell.value is not None else \"\"\n",
    "                if existing_str == \"\":\n",
    "                    error_cell.value = within_row_errors_core[within_idx]\n",
    "                else:\n",
    "                    error_cell.value = existing_str + \",\" + within_row_errors_core[within_idx]\n",
    "            if cell.value in within_row_big_errors:\n",
    "                cell.fill = PatternFill(start_color=\"e06666\", end_color=\"e06666\", fill_type='solid')\n",
    "\n",
    "                within_idx = within_row_big_errors.index(cell.value)\n",
    "                error_cell = ws.cell(row=row_idx, column=max_columns + 1)\n",
    "                existing_str = str(error_cell.value) if error_cell.value is not None else \"\"\n",
    "                if existing_str == \"\":\n",
    "                    error_cell.value = within_row_big_errors_core[within_idx]\n",
    "                else:\n",
    "                    error_cell.value = existing_str + \",\" + within_row_big_errors_core[within_idx]\n",
    "        print(f'Added error corrections for {os.path.basename(excel_path)}')\n",
    "\n",
    "    else:\n",
    "        headers_to_color = headers\n",
    "        start_col = 1\n",
    "\n",
    "    # Define distinct light colors\n",
    "    colors = [\n",
    "        \"FFB3BA\", \"FFDFBA\", \"FFFFBA\", \"BAFFC9\", \"BAE1FF\",\n",
    "        \"D7BAFF\", \"FFC3F7\", \"BAFFD9\", \"FFE0BA\", \"D0BAFF\"\n",
    "    ]\n",
    "\n",
    "    colors_note = [\n",
    "    \"#FFB3BA\", \"#FFDFBA\", \"#FFFFBA\", \"#BAFFC9\", \"#BAE1FF\",\n",
    "    \"#D7BAFF\", \"#FFC3F7\", \"#BAFFD9\", \"#FFE0BA\", \"#D0BAFF\"]\n",
    "\n",
    "    # Assign colors to unique header names\n",
    "    color_map = {}\n",
    "    for col_name in headers_to_color:\n",
    "        if col_name not in color_map:\n",
    "            color_map[col_name] = colors[len(color_map) % len(colors)]\n",
    "\n",
    "    # Apply fill color to each column\n",
    "    col_idx = start_col\n",
    "    for col_name in headers_to_color:\n",
    "\n",
    "        fill = PatternFill(\n",
    "            start_color=color_map[col_name],\n",
    "            end_color=color_map[col_name],\n",
    "            fill_type='solid'\n",
    "        )\n",
    "\n",
    "        # Color header\n",
    "        ws.cell(row=1, column=col_idx).fill = fill\n",
    "\n",
    "        # Color all data rows\n",
    "        for row_idx in range(2, ws.max_row + 1):\n",
    "            ws.cell(row=row_idx, column=col_idx).fill = fill\n",
    "\n",
    "        col_idx += 1\n",
    "\n",
    "    # #Define a thick border on the left side of a column \n",
    "    thick_side = Side(border_style=\"thick\", color=\"000000\") \n",
    "    thick_left_border = Border(left=thick_side)\n",
    "    # Loop through columns starting at column 3 (Excel index), applying thick border every 2 columns \n",
    "    for col_idx in range(2, ws.max_column + 2, 2): # 3, 5, 7, 9 ... \n",
    "        for row_idx in range(1, ws.max_row + 1):\n",
    "            ws.cell(row=row_idx, column=col_idx).border = thick_left_border\n",
    "\n",
    "    # Medium thick on bottom of row 1\n",
    "    medium_border = 1 if index_v else 0\n",
    "    medium_side = Side(border_style=\"medium\", color=\"000000\")\n",
    "    bottom_border = Border(bottom=medium_side)\n",
    "    for col_idx in range(1, ws.max_column + medium_border):\n",
    "        ws.cell(row=1, column=col_idx).border = bottom_border\n",
    "\n",
    "    #freeze top row\n",
    "    ws.freeze_panes = \"A2\"\n",
    "\n",
    "    #add error column header\n",
    "    if index_v:\n",
    "        error_col_cell = ws.cell(row=1, column=max_columns + 1)\n",
    "        error_col_cell.value = \"Error Cores\"\n",
    "        error_col_cell.font = Font(bold=True)\n",
    "        error_col_cell.alignment = Alignment(horizontal=\"center\", vertical=\"center\")\n",
    "\n",
    "    #rename sheet\n",
    "    ws.title = \"Ice Core Depth Comparison\"\n",
    "\n",
    "    #create second page for legend/stats\n",
    "    legend_sheet = wb.create_sheet(title=\"Legend & Stats\")\n",
    "    legend_sheet[\"A1\"] = \"Legend and Stats\"\n",
    "    legend_sheet[\"A1\"].font = Font(size=14, bold=True)\n",
    "    legend_sheet[\"A1\"].alignment = Alignment(horizontal=\"center\")\n",
    "    legend_sheet.merge_cells('A1:D1')  # Merge first row for title\n",
    "\n",
    "    # Add legend entries (example)\n",
    "    legend = {\n",
    "        \"ffd966\": \"Rows flagged with values differing by > 0.1 but all less than < 0.25\",\n",
    "        \"e06666\": \"Rows flagged with values maximum differing by > 0.25\",\n",
    "    }\n",
    "\n",
    "    legend_row = 3\n",
    "    legend_sheet[f\"A{legend_row}\"] = \"Legend\"\n",
    "    legend_sheet[f\"A{legend_row}\"].font = Font(bold=True)\n",
    "\n",
    "    legend_row +=1\n",
    "    for key, desc in legend.items():\n",
    "        cell = legend_sheet[f\"A{legend_row}\"]\n",
    "        cell.fill = PatternFill(start_color=key, end_color=key, fill_type='solid')  # Set fill style\n",
    "        legend_sheet[f\"B{legend_row}\"] = desc\n",
    "        legend_row += 1\n",
    "\n",
    "    # Add some stats (example)\n",
    "    stats = {\n",
    "        \"Total Rows\": ws.max_row - 1,  # assuming ws is your main sheet, -1 for header\n",
    "        \"Total Rows with Errors\": f\"{len(within_row_errors)} ({len(within_row_errors) / (ws.max_row - 1) * 100:.2f}%)\",\n",
    "        \"Total Minor Errors (excl. Major)\": f\"{len(within_row_errors) - len(within_row_big_errors)} ({(len(within_row_errors) - len(within_row_big_errors)) / (ws.max_row - 1) * 100:.2f}%)\",\n",
    "        \"Total Major Errors\": f\"{len(within_row_big_errors)} ({len(within_row_big_errors) / (ws.max_row - 1) * 100:.2f}%)\",\n",
    "        \"Minimum Columns per Row\": f\"{min_cols_per[excel_path]}\",\n",
    "    }\n",
    "\n",
    "    stats_row = legend_row + len(legend)\n",
    "    legend_sheet[f\"A{stats_row}\"] = \"Statistics\"\n",
    "    legend_sheet[f\"A{stats_row}\"].font = Font(bold=True)\n",
    "\n",
    "    stats_row +=1\n",
    "    for stat, val in stats.items():\n",
    "        legend_sheet[f\"A{stats_row}\"] = stat\n",
    "        legend_sheet[f\"B{stats_row}\"] = val\n",
    "        stats_row += 1\n",
    "\n",
    "    #adjust readability\n",
    "    legend_sheet.column_dimensions['A'].width = 25  # wider column A in legend\n",
    "\n",
    "    # Save workbook\n",
    "    wb.save(excel_path)\n",
    "    print(f\"Styled and saved workbook at {excel_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bicc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
