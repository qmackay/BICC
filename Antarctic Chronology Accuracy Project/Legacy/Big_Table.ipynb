{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import os\n",
    "import yaml\n",
    "import itertools\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from openpyxl.styles import PatternFill\n",
    "from openpyxl import load_workbook\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DF-EDC',\n",
       " 'DF-EDML',\n",
       " 'DF-NGRIP',\n",
       " 'DF-GRIP',\n",
       " 'DF-WDC',\n",
       " 'DF-GISP2',\n",
       " 'EDC-EDML',\n",
       " 'EDC-NGRIP',\n",
       " 'EDC-GRIP',\n",
       " 'EDC-WDC',\n",
       " 'EDC-GISP2',\n",
       " 'EDML-NGRIP',\n",
       " 'EDML-GRIP',\n",
       " 'EDML-WDC',\n",
       " 'EDML-GISP2',\n",
       " 'NGRIP-GRIP',\n",
       " 'NGRIP-WDC',\n",
       " 'NGRIP-GISP2',\n",
       " 'GRIP-WDC',\n",
       " 'GRIP-GISP2',\n",
       " 'WDC-GISP2']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading all BICC data, does NOT include anything else (incl. the removed GISP2-GRIP)\n",
    "\n",
    "#load cores list from params\n",
    "params = f'/Users/quinnmackay/Documents/GitHub/BICC/Paleochrono BICC Work/Paleochrono BICC Experiment/BICC2025/parameters.yml'\n",
    "with open(params, 'r') as f:\n",
    "    first_line = f.readline()\n",
    "params_load = yaml.safe_load(first_line)\n",
    "list_sites = params_load['list_sites']\n",
    "\n",
    "# get all link combos\n",
    "pairs = [f\"{a}-{b}\" for a, b in itertools.combinations(list_sites, 2)]\n",
    "\n",
    "pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_table = pd.DataFrame()\n",
    "error_margin = 0.2\n",
    "big_table['error'] = ''\n",
    "\n",
    "for core in list_sites: # loop through each core\n",
    "    for comparison_core in list_sites: # loop through each core other than the initial load\n",
    "        pair = f\"{core}-{comparison_core}\"\n",
    "        if core != comparison_core and pair in pairs: # make sure not the same core and we skip non-existent linkages\n",
    "            file = Path(f'/Users/quinnmackay/Documents/GitHub/BICC/Paleochrono BICC Work/Paleochrono BICC Experiment/BICC2025/{pair}/iceice_synchro_horizons.txt')\n",
    "\n",
    "            with open(file, 'r') as f:\n",
    "                skip = 0\n",
    "                for line in f:\n",
    "                    if line.startswith('#'):\n",
    "                        skip += 1\n",
    "                    else:\n",
    "                        break\n",
    "\n",
    "            load_data = pd.read_csv(file, sep='\\t', comment='#', skiprows=skip+1, names=[core, comparison_core], usecols=[0,1])\n",
    "\n",
    "            if core == list_sites[0] and comparison_core == list_sites[1]:\n",
    "                big_table = load_data.copy()\n",
    "                continue\n",
    "        \n",
    "            if core in big_table.columns and comparison_core not in big_table.columns:\n",
    "                for idx, row in load_data.iterrows():\n",
    "                    depth_core = row[core]\n",
    "\n",
    "                    # Compute absolute distance from current depth\n",
    "                    diff = (big_table[core] - depth_core).abs()\n",
    "\n",
    "                    # Mask for rows within ±0.1\n",
    "                    mask = diff <= 0.1\n",
    "\n",
    "                    # If any row is close enough, pick the closest and update it\n",
    "                    if mask.any():\n",
    "                        closest_idx = diff[mask].idxmin()  # index of closest match\n",
    "                        big_table.loc[closest_idx, comparison_core] = row[comparison_core]\n",
    "\n",
    "                    if not mask.any():\n",
    "                        # If no close match, append new row\n",
    "                        new_row = pd.Series({core: depth_core, comparison_core: row[comparison_core]})\n",
    "                        big_table = pd.concat([big_table, new_row.to_frame().T], ignore_index=True)\n",
    "                continue\n",
    "                \n",
    "            if core in big_table.columns and comparison_core in big_table.columns:\n",
    "                for idx, row in load_data.iterrows():\n",
    "                    depth1_core = row[core]\n",
    "                    depth2_core = row[comparison_core]\n",
    "\n",
    "                    # Compute absolute distance from current depth\n",
    "                    diff1 = (big_table[core] - depth1_core).abs()\n",
    "                    diff2 = (big_table[comparison_core] - depth2_core).abs()\n",
    "\n",
    "                    # Mask for rows within ±0.1\n",
    "                    mask1 = diff1 <= 0.1\n",
    "                    mask2 = diff2 <= 0.1\n",
    "\n",
    "                    # If any row is close enough, pick the closest and update it\n",
    "                    if mask1.any() and mask2.any():\n",
    "                        closest_idx1 = diff1[mask1].idxmin()  # index of closest match\n",
    "                        closest_idx2 = diff2[mask2].idxmin()  # index of closest match\n",
    "\n",
    "                        if closest_idx1 != closest_idx2:\n",
    "                            if pd.isna(big_table.loc[closest_idx1, comparison_core]):\n",
    "                                big_table.loc[closest_idx1, comparison_core] = row[comparison_core]\n",
    "                            if pd.isna(big_table.loc[closest_idx2, core]):\n",
    "                                big_table.loc[closest_idx2, core] = row[core]\n",
    "                            continue\n",
    "\n",
    "                        if closest_idx1 == closest_idx2:\n",
    "                            #existing match\n",
    "                            big_table.loc[closest_idx1, 'error'] = 'multi-core validated'\n",
    "                            continue\n",
    "                    \n",
    "                    if mask1.any() != mask2.any():\n",
    "                        if mask1.any():\n",
    "                            closest_idx = diff1[mask1].idxmin()  # index of closest match\n",
    "                            big_table.loc[closest_idx, comparison_core] = row[comparison_core]\n",
    "                        if mask2.any():\n",
    "                            closest_idx = diff2[mask2].idxmin()  # index of closest match\n",
    "                            big_table.loc[closest_idx, core] = row[core]\n",
    "                        continue\n",
    "\n",
    "                    if not mask1.any() and not mask2.any():\n",
    "                        # If no close match, append new row\n",
    "                        new_row = pd.Series({core: row[core], comparison_core: row[comparison_core]})\n",
    "                        big_table = pd.concat([big_table, new_row.to_frame().T], ignore_index=True)\n",
    "                        continue\n",
    "\n",
    "                continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of row merges: 109\n",
      "Number of changes to 'multi-core validated': 63\n"
     ]
    }
   ],
   "source": [
    "big_table.reset_index(drop=True, inplace=True)\n",
    "out=0\n",
    "change=0\n",
    "for core in list_sites: #do each core\n",
    "    for idx, row in big_table.iterrows(): #do each row for each core\n",
    "\n",
    "        if idx not in big_table.index:\n",
    "            continue\n",
    "\n",
    "        depth_core = row[core]\n",
    "        diff = (big_table[core] - depth_core).abs()\n",
    "        mask = diff <= 0.1 #make the mask for rows within 0.1\n",
    "        mask[idx] = False #exclude self\n",
    "\n",
    "        if mask.any(): #if any true\n",
    "            runs = mask.sum() #number of trues\n",
    "            for z in range(runs): #do for each true\n",
    "                \n",
    "                diff_filtered = diff[mask]\n",
    "                if len(diff_filtered) == 0:\n",
    "                    break\n",
    "                closest_idx = diff_filtered.idxmin() #index of closest match\n",
    "\n",
    "                if closest_idx not in big_table.index:\n",
    "                    mask[closest_idx] = False\n",
    "                    continue\n",
    "\n",
    "                for col in row.index: #for each column in the original row once finding a match\n",
    "                    if pd.isna(big_table.loc[idx, col]) and not pd.isna(big_table.loc[closest_idx, col]):\n",
    "                        big_table.loc[idx, col] = big_table.loc[closest_idx, col]\n",
    "                \n",
    "                if big_table.loc[closest_idx, 'error'] == 'multi-core validated':\n",
    "                    change+=1\n",
    "                    big_table.loc[idx, 'error'] = 'multi-core validated'\n",
    "                \n",
    "                out+=1\n",
    "                mask[closest_idx] = False\n",
    "                big_table = big_table.drop(index=closest_idx)\n",
    "\n",
    "print(f'Number of row merges: {out}')\n",
    "print(f\"Number of changes to 'multi-core validated': {change}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_path = '/Users/quinnmackay/Desktop/temp/big_bicc_table.xlsx'\n",
    "\n",
    "sorted_big_table = big_table.sort_values(by=list_sites[0]).reset_index(drop=True)\n",
    "sorted_big_table.to_excel(f'{excel_path}', index=False)\n",
    "\n",
    "# error coloring\n",
    "\n",
    "error_colors = {\n",
    "    'new_row_from_separate_matches': PatternFill(start_color='FF0000', end_color='FF0000', fill_type='solid'),      # Red\n",
    "    'new_row_from_partial': PatternFill(start_color='ffbb00', end_color='ffbb00', fill_type='solid'),           # Light Red\n",
    "    'multi-core validated': PatternFill(start_color='00FF00', end_color='00FF00', fill_type='solid'),          # Green\n",
    "}\n",
    "\n",
    "# Load workbook\n",
    "wb = load_workbook(excel_path)\n",
    "ws = wb.active\n",
    "\n",
    "# Find 'error' column index (1-based for openpyxl)\n",
    "error_col_idx = list(big_table.columns).index('error') + 1\n",
    "\n",
    "# Iterate over rows (skip header)\n",
    "for row_idx in range(2, ws.max_row + 1):\n",
    "    cell = ws.cell(row=row_idx, column=error_col_idx)\n",
    "    error_type = cell.value\n",
    "    if error_type and error_type in error_colors:\n",
    "        fill = error_colors[error_type]\n",
    "        # Color all cells in row except error column\n",
    "        for col_idx in range(1, ws.max_column + 1):\n",
    "            if col_idx != error_col_idx:\n",
    "                ws.cell(row=row_idx, column=col_idx).fill = fill\n",
    "\n",
    "ws.freeze_panes = 'A2'\n",
    "wb.save(excel_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_path = '/Users/quinnmackay/Desktop/temp/3_min_big_bicc_table.xlsx'\n",
    "\n",
    "sorted_big_table = sorted_big_table.dropna(thresh=3, axis=0)\n",
    "sorted_big_table = sorted_big_table.sort_values(by=list_sites[0]).reset_index(drop=True)\n",
    "sorted_big_table.to_excel(f'{excel_path}', index=False)\n",
    "\n",
    "# error coloring\n",
    "\n",
    "error_colors = {\n",
    "    'new_row_from_separate_matches': PatternFill(start_color='FF0000', end_color='FF0000', fill_type='solid'),      # Red\n",
    "    'new_row_from_partial': PatternFill(start_color='ffbb00', end_color='ffbb00', fill_type='solid'),           # Light Red\n",
    "    'multi-core validated': PatternFill(start_color='00FF00', end_color='00FF00', fill_type='solid'),          # Green\n",
    "}\n",
    "\n",
    "# Load workbook\n",
    "wb = load_workbook(excel_path)\n",
    "ws = wb.active\n",
    "\n",
    "# Find 'error' column index (1-based for openpyxl)\n",
    "error_col_idx = list(big_table.columns).index('error') + 1\n",
    "\n",
    "# Iterate over rows (skip header)\n",
    "for row_idx in range(2, ws.max_row + 1):\n",
    "    cell = ws.cell(row=row_idx, column=error_col_idx)\n",
    "    error_type = cell.value\n",
    "    if error_type and error_type in error_colors:\n",
    "        fill = error_colors[error_type]\n",
    "        # Color all cells in row except error column\n",
    "        for col_idx in range(1, ws.max_column + 1):\n",
    "            if col_idx != error_col_idx:\n",
    "                ws.cell(row=row_idx, column=col_idx).fill = fill\n",
    "\n",
    "ws.freeze_panes = 'A2'\n",
    "wb.save(excel_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bicc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
