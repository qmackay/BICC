{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import os\n",
    "import yaml\n",
    "import itertools\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from openpyxl.styles import PatternFill\n",
    "from openpyxl import load_workbook\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "from openpyxl.styles import Border, Side\n",
    "from openpyxl.styles import Alignment, Font\n",
    "from openpyxl.utils import get_column_letter\n",
    "from numpy._core.numeric import indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cores list of sites for AA\n",
    "\n",
    "project = 'Antarctic'\n",
    "output_dir = '/Users/quinnmackay/Desktop/table_out'\n",
    "\n",
    "# get all link combos\n",
    "with open(f'/Users/quinnmackay/Documents/GitHub/BICC/Antarctic Chronology Accuracy Project/{project}/parameters.yml') as f:\n",
    "    data = yaml.safe_load(f)\n",
    "list_sites = data[\"list_sites\"]\n",
    "pairs = [f\"{a}-{b}\" for a, b in itertools.combinations(list_sites, 2)]\n",
    "\n",
    "error_margin = 0.1\n",
    "big_error_margin = 0.25\n",
    "minor_depth_inversion = 0.001\n",
    "major_depth_inversion = 0.5\n",
    "base_core_age = 'EDC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create each segment\n",
    "segment1 = np.arange(0.01, 0.1, 0.01)  # 0.01 to 0.09 by 0.01\n",
    "segment2 = np.arange(0.1, 0.2, 0.02)   # 0.1 to 0.18 by 0.02\n",
    "segment3 = np.arange(0.2, 0.51, 0.05)  # 0.2 to 0.5 by 0.05\n",
    "\n",
    "# Concatenate them\n",
    "bounds = np.concatenate([segment1, segment2, segment3])\n",
    "\n",
    "total_errors = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed pair EDC-WDC, total points after merging: 714, (897 original total rows)\n",
      "WARNING: Row 791.874 | 1454.36 for EDC-EDML. Reference Severi2007_Ruth2007.\n",
      "Called by row 791.88 | 1454.37 from reference AICC2012_Links.\n",
      "Processed pair EDC-EDML, total points after merging: 555, (828 original total rows)\n",
      "Processed pair EDC-DF, total points after merging: 1555, (1605 original total rows)\n",
      "Processed pair EDC-TALDICE, total points after merging: 232, (246 original total rows)\n",
      "Processed pair WDC-EDML, total points after merging: 1117, (1315 original total rows)\n",
      "Processed pair WDC-DF, total points after merging: 845, (1078 original total rows)\n",
      "Processed pair WDC-TALDICE, total points after merging: 797, (1219 original total rows)\n",
      "Processed pair EDML-DF, total points after merging: 215, (215 original total rows)\n",
      "Processed pair EDML-TALDICE, total points after merging: 128, (128 original total rows)\n",
      "Processed pair DF-TALDICE, total points after merging: 111, (111 original total rows)\n",
      "total updates made: 18269 (+18269)\n",
      "total updates made: 20663 (+2394)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 157\u001b[39m\n\u001b[32m    154\u001b[39m matching_indices = diff[diff <= error_margin].index  \u001b[38;5;66;03m# rows that agree within margin\u001b[39;00m\n\u001b[32m    156\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m match_idx \u001b[38;5;129;01min\u001b[39;00m matching_indices:  \u001b[38;5;66;03m# for each compatible row\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m     col_updates[match_idx] = \u001b[43mbig_table\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mat\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# schedule update for col\u001b[39;00m\n\u001b[32m    158\u001b[39m     match_updates[match_idx] = big_table[match_col].at[index]  \u001b[38;5;66;03m# schedule update for match_col\u001b[39;00m\n\u001b[32m    159\u001b[39m     ref_updates[match_idx] = big_table[ref_col].at[index]  \u001b[38;5;66;03m# schedule update for reference column\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/bicc/lib/python3.13/site-packages/pandas/core/indexing.py:2575\u001b[39m, in \u001b[36m_AtIndexer.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   2572\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mInvalid call for scalar access (getting)!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2573\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.obj.loc[key]\n\u001b[32m-> \u001b[39m\u001b[32m2575\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/bicc/lib/python3.13/site-packages/pandas/core/indexing.py:2527\u001b[39m, in \u001b[36m_ScalarAccessIndexer.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   2524\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mInvalid call for scalar access (getting)!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2526\u001b[39m key = \u001b[38;5;28mself\u001b[39m._convert_key(key)\n\u001b[32m-> \u001b[39m\u001b[32m2527\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtakeable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_takeable\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/bicc/lib/python3.13/site-packages/pandas/core/series.py:1220\u001b[39m, in \u001b[36mSeries._get_value\u001b[39m\u001b[34m(self, label, takeable)\u001b[39m\n\u001b[32m   1217\u001b[39m     new_mgr = \u001b[38;5;28mself\u001b[39m._mgr.get_rows_with_mask(indexer)\n\u001b[32m   1218\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._constructor_from_mgr(new_mgr, axes=new_mgr.axes).__finalize__(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1220\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_value\u001b[39m(\u001b[38;5;28mself\u001b[39m, label, takeable: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m   1221\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1222\u001b[39m \u001b[33;03m    Quickly retrieve single value at passed index label.\u001b[39;00m\n\u001b[32m   1223\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1231\u001b[39m \u001b[33;03m    scalar value\u001b[39;00m\n\u001b[32m   1232\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   1233\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m takeable:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for error_margin in bounds:\n",
    "\n",
    "    big_table = pd.DataFrame()\n",
    "    all_links_count = {}\n",
    "    all_links_foragesort = {}\n",
    "\n",
    "    if base_core_age not in list_sites:\n",
    "        print('Incorrect base_core set for sorting')\n",
    "        sys.exit()\n",
    "\n",
    "    for core in list_sites: # loop through each core\n",
    "        for comparison_core in list_sites: # loop through each core other than the initial load\n",
    "            pair = f\"{core}-{comparison_core}\"\n",
    "            if core != comparison_core and pair in pairs: # make sure not the same core and we skip non-existent linkages\n",
    "                pair_dir = Path(f'/Users/quinnmackay/Documents/GitHub/BICC/Antarctic Chronology Accuracy Project/{project}/{pair}')\n",
    "\n",
    "                # Check: directory exists AND contains at least one .txt file\n",
    "                txt_files = list(pair_dir.glob(\"*.txt\"))\n",
    "                if not pair_dir.is_dir() or not txt_files:\n",
    "                    continue\n",
    "\n",
    "                dfs=[] #load all text files into one\n",
    "                for txt in txt_files:\n",
    "                    df = pd.read_csv(txt, sep=\"\\t\", comment=\"#\")\n",
    "                    dfs.append(df)\n",
    "        \n",
    "                num_files = len(dfs)\n",
    "                load_data = pd.concat(dfs, ignore_index=True)\n",
    "                original_rows = len(load_data)\n",
    "\n",
    "                drop_rows = []\n",
    "                drop_rows_merge = set()\n",
    "                new_merged_rows = []\n",
    "                for idx, row in load_data.iterrows():\n",
    "\n",
    "                    mask1 = abs(row['depth1'] - load_data['depth1']) <= error_margin\n",
    "                    mask1[idx] = False\n",
    "                    mask2 = abs(row['depth2'] - load_data['depth2']) <= error_margin \n",
    "                    mask2[idx] = False\n",
    "\n",
    "                    close_points = load_data[mask1 & mask2]\n",
    "                    num_close = len(close_points)\n",
    "                    close_idxs = load_data.index[mask1 & mask2]\n",
    "\n",
    "                    if num_close > 0:\n",
    "                        refs = [load_data.at[idx, 'reference']] + [load_data.at[i, 'reference'] for i in close_idxs] #adjoin references\n",
    "                        merged_ref = \"; \".join(str(r) for r in refs if pd.notna(r))\n",
    "\n",
    "                        depth1_vals = [load_data.at[idx, 'depth1']] + [load_data.at[i, 'depth1'] for i in close_idxs]\n",
    "                        merged_depth1 = np.mean(depth1_vals)\n",
    "\n",
    "                        depth2_vals = [load_data.at[idx, 'depth2']] + [load_data.at[i, 'depth2'] for i in close_idxs]\n",
    "                        merged_depth2 = np.mean(depth2_vals)\n",
    "\n",
    "                        new_merged_rows.append({'reference': merged_ref, 'depth1': merged_depth1, 'depth2': merged_depth2}) #create new merged row\n",
    "\n",
    "                        drop_rows_merge.add(idx)\n",
    "                        for i in close_idxs:\n",
    "                            drop_rows.append(i)\n",
    "                            if drop_rows.count(i) >= num_files:\n",
    "                                continue\n",
    "\n",
    "                # drop duplicate rows\n",
    "                drop_rows = set(drop_rows).union(drop_rows_merge)\n",
    "                load_data = load_data.drop(index=drop_rows).reset_index(drop=True)\n",
    "                # add merged rows\n",
    "                merged_df = pd.DataFrame(new_merged_rows)\n",
    "                load_data = pd.concat([load_data, merged_df], ignore_index=True)\n",
    "                load_data.drop_duplicates(subset=['depth1', 'depth2'], inplace=True)\n",
    "                load_data = load_data.reset_index(drop=True)\n",
    "\n",
    "                load_data = load_data.sort_values(by=['depth1']).reset_index(drop=True)\n",
    "            \n",
    "                #set up pair code stuff\n",
    "                load_data[f\"{pair}_code\"] = [f\"{pair}_{idx}\" for idx in load_data.index]\n",
    "\n",
    "                if core or comparison_core == base_core_age: # get all links attached to base core for age sorting\n",
    "                    if core == base_core_age:\n",
    "                        all_links_count[comparison_core] = len(load_data)\n",
    "                        all_links_foragesort[comparison_core] = load_data[['depth1', 'depth2']].copy() #get all links for sorting the age, all links attached to base core\n",
    "                        all_links_foragesort[comparison_core].rename(columns={f'depth1': base_core_age, 'depth2': 'comparison_core'}, inplace=True)\n",
    "                    elif comparison_core == base_core_age:\n",
    "                        all_links_count[core] = len(load_data)\n",
    "                        all_links_foragesort[core] = load_data[['depth1', 'depth2']].copy()\n",
    "                        all_links_foragesort[core].rename(columns={f'depth2': base_core_age, 'depth1': 'comparison_core'}, inplace=True)\n",
    "\n",
    "                # rename to create unique columns for this pair\n",
    "                load_data = load_data.rename(columns={\n",
    "                    'depth1': f\"{pair}_{core}\",\n",
    "                    'depth2': f\"{pair}_{comparison_core}\",\n",
    "                    'reference': f\"{pair}_reference\",\n",
    "                })\n",
    "\n",
    "                # append rows (block)\n",
    "                big_table = pd.concat([big_table, load_data],\n",
    "                                    axis=0,\n",
    "                                    ignore_index=True)\n",
    "\n",
    "    # make all_links_count the largest value\n",
    "    all_links_count[base_core_age] = sum(all_links_count.values())\n",
    "\n",
    "    core_groups = defaultdict(list)\n",
    "    matching_groups = defaultdict(list)\n",
    "\n",
    "    link_columns_temp = [col for col in big_table.columns if \"reference\" not in col] #remove ref cols\n",
    "    link_columns = [col for col in link_columns_temp if \"code\" not in col] #remove code cols\n",
    "\n",
    "    for col in link_columns:\n",
    "        suffix = col.split(\"_\")[-1]\n",
    "        core_groups[suffix].append(col) #group cols by suffix\n",
    "        \n",
    "        match = col.split(\"_\")[0]\n",
    "        core1 = match.split(\"-\")[0]\n",
    "        core2 = match.split(\"-\")[1]\n",
    "\n",
    "        if core1 == suffix:\n",
    "            matching_core = core2\n",
    "        elif core2 == suffix:\n",
    "            matching_core = core1\n",
    "\n",
    "        matching_groups[suffix].append(f\"{match}_{matching_core}\")\n",
    "\n",
    "    update_check = 0  # total number of filled-in values across all passes\n",
    "    refresh = 1  # triggers loop until no new updates are found\n",
    "    while refresh > 0:  # keep looping as long as new values were added\n",
    "        refresh = 0  # reset per loop\n",
    "        for core, assoc_cols in core_groups.items():  # columns associated with each core\n",
    "            matching_cols = matching_groups[core]  # corresponding matching columns\n",
    "\n",
    "            for col, match_col in zip(assoc_cols, matching_cols):  # pair actual vs matching column\n",
    "                base_col = col.split(\"_\")[0] # get base link name for reference column\n",
    "                i_core1 = base_col.split(\"-\")[0] #get core names for index updates\n",
    "                i_core2 = base_col.split(\"-\")[1]\n",
    "                ref_col = f\"{base_col}_reference\"  # reference column name\n",
    "                code_col = f\"{base_col}_code\"\n",
    "                for col_check in assoc_cols:  # compare against all other columns of same core\n",
    "                    if col == col_check:  # skip self-comparison\n",
    "                        continue\n",
    "\n",
    "                    col_updates = {}  # values to update in primary column\n",
    "                    match_updates = {}  # values to update in matching column\n",
    "                    ref_updates = {}  # values to update in reference column\n",
    "                    code_updates = {}\n",
    "\n",
    "                    for index, value in big_table[col].items():  # loop over each row\n",
    "                        try:\n",
    "                            diff = (big_table[col_check] - value).abs()  # compute absolute diff\n",
    "                        except:\n",
    "                            print(f'Error computing diff for {col} and {col_check} at index {index} with value {value}')\n",
    "                            print(core_groups.items())\n",
    "                            sys.exit()\n",
    "                        matching_indices = diff[diff <= error_margin].index  # rows that agree within margin\n",
    "\n",
    "                        for match_idx in matching_indices:  # for each compatible row\n",
    "                            col_updates[match_idx] = big_table[col].at[index]  # schedule update for col\n",
    "                            match_updates[match_idx] = big_table[match_col].at[index]  # schedule update for match_col\n",
    "                            ref_updates[match_idx] = big_table[ref_col].at[index]  # schedule update for reference column\n",
    "                            code_updates[match_idx] = big_table[code_col].at[index]\n",
    "                \n",
    "                    for match_idx, new_val in col_updates.items():  # apply col updates\n",
    "                        if pd.isna(big_table.at[match_idx, col]):  # only fill empty cells\n",
    "                            big_table.at[match_idx, col] = new_val  # write new value\n",
    "                            update_check+=1  # count total updates\n",
    "                            refresh+=1  # signal another full loop is needed\n",
    "                    for match_idx, new_val in match_updates.items():  # apply match_col updates\n",
    "                        if pd.isna(big_table.at[match_idx, match_col]):  # only fill empty cells\n",
    "                            big_table.at[match_idx, match_col] = new_val  # write new value\n",
    "                    for match_idx, new_val in ref_updates.items():  # apply reference column updates\n",
    "                        if pd.isna(big_table.at[match_idx, ref_col]):  # only fill empty cells\n",
    "                            big_table.at[match_idx, ref_col] = new_val  # write new value\n",
    "                    for match_idx, new_val in code_updates.items():  # apply reference column updates\n",
    "                        if pd.isna(big_table.at[match_idx, code_col]):  # only fill empty cells\n",
    "                            big_table.at[match_idx, code_col] = new_val  # write new value\n",
    "                \n",
    "        print(f'total updates made: {update_check} (+{refresh})')  # show total and new updates this pass\n",
    "\n",
    "    #deal with with duplicates\n",
    "    non_ref_code_cols = [c for c in big_table.columns if \"reference\" not in c and \"code\" not in c]\n",
    "    big_table[non_ref_code_cols] = big_table[non_ref_code_cols].round(8)\n",
    "    duplicates_mask = big_table.duplicated(subset=non_ref_code_cols, keep='first')\n",
    "    num_dupe = duplicates_mask.sum()\n",
    "    big_table_cleaned = big_table.drop_duplicates(subset=non_ref_code_cols, keep='first').reset_index(drop=True)\n",
    "    print(f'Reduced table by {num_dupe/len(big_table)*100:.2f}% due to duplicates')\n",
    "\n",
    "    #reorganize based on ages\n",
    "    core_chron = {}\n",
    "    for core, columns in core_groups.items():\n",
    "        file_path = f'/Users/quinnmackay/Documents/GitHub/BICC/Antarctic Chronology Accuracy Project/{project}/Chronologies/{core}.txt'\n",
    "        df = pd.read_csv(file_path, sep=\"\\t\", comment=\"#\", names=['depth', 'age']).sort_values(by=['depth']).reset_index(drop=True)\n",
    "        core_chron[core] = df\n",
    "\n",
    "    chron_df = core_chron[base_core_age]\n",
    "    for index, row in big_table_cleaned.iterrows():\n",
    "        for core, columns in sorted(core_groups.items(),\n",
    "                                key=lambda x: all_links_count[x[0]],\n",
    "                                reverse=True): # For each core and its associated list of column names\n",
    "            values = [] # Collect the values for this core on this row\n",
    "            values = [row[col] for col in columns if not pd.isna(row[col])] # Remove NaN values so they don't interfere with comparison, get values for this core on this row\n",
    "            if len(values) >= 1 and core == base_core_age:\n",
    "                avg_depth = np.mean(values)\n",
    "                age_core_row = np.interp(avg_depth, chron_df['depth'], chron_df['age'])\n",
    "                big_table_cleaned.at[index, 'estimated_age'] = age_core_row\n",
    "            elif len(values) >= 1 and pd.isna(big_table_cleaned.at[index, 'estimated_age']): #skip ones already done \n",
    "                avg_depth = np.mean(values)\n",
    "                equiv_depth = np.interp(avg_depth, all_links_foragesort[core]['comparison_core'], all_links_foragesort[core][base_core_age])\n",
    "                age_core_row = np.interp(equiv_depth, chron_df['depth'], chron_df['age'])\n",
    "                big_table_cleaned.at[index, 'estimated_age'] = age_core_row\n",
    "\n",
    "    big_table_cleaned = big_table_cleaned.sort_values(by=['estimated_age']).reset_index(drop=True)\n",
    "    big_table_cleaned = big_table_cleaned.drop(columns=['estimated_age'])\n",
    "\n",
    "    # Do evaluation for errors (inter-row errors)\n",
    "    within_row_errors = []\n",
    "    within_row_errors_core = []\n",
    "    within_row_big_errors = []\n",
    "    within_row_big_errors_core = []\n",
    "    for index, row in big_table_cleaned.iterrows(): # Iterate over every row in the table\n",
    "        for core, columns in core_groups.items(): # For each core and its associated list of column names\n",
    "            values = []\n",
    "            for col in columns: # Collect the values for this core on this row\n",
    "                values.append(row[col])\n",
    "            values = [v for v in values if not pd.isna(v)] # Remove NaN values so they don't interfere with comparison\n",
    "            if len(values) >= 2:\n",
    "                diff = abs(max(values) - min(values))\n",
    "                if diff >= error_margin:\n",
    "                    within_row_errors.append(index)\n",
    "                    within_row_errors_core.append(core)\n",
    "                    #print(f\"Row {index} core {core} values: {values} diff={diff}\")\n",
    "                if diff >= big_error_margin and index not in within_row_big_errors:\n",
    "                    within_row_big_errors.append(index)\n",
    "\n",
    "    # Do evaluation for depth inversion errors \n",
    "    depth_inversion_errors = []\n",
    "    depth_inversion_errors_core = []\n",
    "    depth_inversion_errors_major = []\n",
    "    for core, columns in core_groups.items():  \n",
    "        for col in columns:  # Check each depth column for this core\n",
    "            # Extract the column's non-NaN values with their index\n",
    "            series = big_table_cleaned[col].dropna()\n",
    "            # Compare each depth with the previous one in sorted index order\n",
    "            inversion_detect = False\n",
    "            last_clean = series.iloc[0]  # Initialize last clean depth value\n",
    "            for (prev_idx, prev_val), (curr_idx, curr_val) in zip(series.items(), list(series.items())[1:]):\n",
    "                \n",
    "                if curr_val > prev_val and inversion_detect==False:\n",
    "                    last_clean = curr_val\n",
    "\n",
    "                if inversion_detect==True:\n",
    "                    if curr_val >= prev_val and curr_val >= last_clean:\n",
    "                        inversion_detect=False\n",
    "                        last_clean = curr_val\n",
    "                    elif curr_val >= prev_val and curr_val < last_clean-minor_depth_inversion: #capture still inverted values just above previous error\n",
    "                        depth_inversion_errors.append(curr_idx)\n",
    "                        depth_inversion_errors_core.append(core)\n",
    "                \n",
    "                if curr_val < prev_val-minor_depth_inversion:  # inversion detected\n",
    "                    depth_inversion_errors.append(curr_idx)\n",
    "                    depth_inversion_errors_core.append(core)\n",
    "                    inversion_detect=True\n",
    "\n",
    "                if curr_val < prev_val-major_depth_inversion and curr_idx not in depth_inversion_errors_major:\n",
    "                    depth_inversion_errors_major.append(curr_idx)\n",
    "\n",
    "    #move cols around\n",
    "    reference_cols = [c for c in big_table_cleaned.columns if \"reference\" in c]\n",
    "    code_cols = [c for c in big_table_cleaned.columns if \"code\" in c]\n",
    "    other_cols = [c for c in big_table_cleaned.columns if \"reference\" not in c and \"code\" not in c]\n",
    "    big_table_cleaned = big_table_cleaned[other_cols + reference_cols + code_cols]\n",
    "\n",
    "    rename_map = {}\n",
    "    for suffix, cols in core_groups.items():\n",
    "        for col in cols:\n",
    "            rename_map[col] = suffix  # rename to suffix only\n",
    "    big_table_cleaned.rename(columns=rename_map, inplace=True)\n",
    "\n",
    "    total_errors.append(len(within_row_errors))\n",
    "    print('----------')\n",
    "    print(f'Total within-row errors at error margin {error_margin}: {len(within_row_errors)}')\n",
    "    print('----------')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bicc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
