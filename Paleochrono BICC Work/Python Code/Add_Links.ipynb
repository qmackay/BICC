{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import os\n",
    "import yaml\n",
    "import itertools\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "\n",
    "# # load svensson links\n",
    "# sv_links = pd.read_excel('/Users/quinnmackay/Documents/GitHub/BICC/Data Storage/Tiepoints/Svensson Links NGRIP-NEEM-GIP-GISP-EDML-EDC-WD-DJ-TALDICE.xls',\n",
    "#                          sheet_name=1, nrows=305, usecols=[0,2,3,4,5,6,7,], names=['NGRIP', 'GRIP', 'GISP2', 'EDML', 'EDC', 'WDC', 'DF'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GISP2</th>\n",
       "      <th>GRIP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>481.03342</td>\n",
       "      <td>455.32385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>774.52980</td>\n",
       "      <td>736.45860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>978.10284</td>\n",
       "      <td>931.79900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1178.74770</td>\n",
       "      <td>1126.60660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1279.60390</td>\n",
       "      <td>1225.09230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1326.02010</td>\n",
       "      <td>1269.44230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1413.77220</td>\n",
       "      <td>1354.70860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1439.71700</td>\n",
       "      <td>1380.48860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        GISP2        GRIP\n",
       "0   481.03342   455.32385\n",
       "1   774.52980   736.45860\n",
       "2   978.10284   931.79900\n",
       "3  1178.74770  1126.60660\n",
       "4  1279.60390  1225.09230\n",
       "5  1326.02010  1269.44230\n",
       "6  1413.77220  1354.70860\n",
       "7  1439.71700  1380.48860\n",
       "8         NaN         NaN"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sv_links = pd.read_excel('/Users/quinnmackay/Documents/GitHub/BICC/Data Storage/Tiepoints/Miyaki_Events_Sigl.xlsx', sheet_name=4, skiprows=1, usecols=[6,7], names=['GISP2', 'GRIP'])\n",
    "comment_add = ['#Includes Miyaki data from Sigl. Original email in Oct. 2025']\n",
    "sv_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backups not enabled.\n"
     ]
    }
   ],
   "source": [
    "backups=False\n",
    "\n",
    "if backups == True:\n",
    "    # add backup of BICC\n",
    "\n",
    "    source_folder = '/Users/quinnmackay/Documents/GitHub/BICC/Paleochrono BICC Work/Paleochrono BICC Experiment/BICC2025'\n",
    "    backup_root = '/Users/quinnmackay/Documents/GitHub/BICC/Paleochrono BICC Work/Paleochrono BICC Experiment/Backups'\n",
    "\n",
    "    # Create timestamp\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    backup_folder = os.path.join(backup_root, f'BICC_Backup_{timestamp}')\n",
    "\n",
    "    # Make sure the backup root exists\n",
    "    os.makedirs(backup_root, exist_ok=True)\n",
    "\n",
    "    # Copy the folder\n",
    "    shutil.copytree(source_folder, backup_folder)\n",
    "\n",
    "    print(f\"Backup completed: {backup_folder}\")\n",
    "else:\n",
    "    print(\"Backups not enabled.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load cores list from params\n",
    "params = '/Users/quinnmackay/Documents/GitHub/BICC/Paleochrono BICC Work/Paleochrono BICC Experiment/BICC2025/parameters.yml'\n",
    "with open(params, 'r') as f:\n",
    "    first_line = f.readline()\n",
    "params_load = yaml.safe_load(first_line)\n",
    "list_sites = params_load['list_sites']\n",
    "\n",
    "# get all link combos\n",
    "pairs = [f\"{a}-{b}\" for a, b in itertools.combinations(list_sites, 2)]\n",
    "\n",
    "#get all combos possible from the svensson links\n",
    "subset = list(sv_links.columns)\n",
    "valid_pairs = [p for p in pairs if all(site in subset for site in p.split('-'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded GRIP-GISP2: 822 rows\n"
     ]
    }
   ],
   "source": [
    "#now, create all the link files\n",
    "sv_synchros = {}\n",
    "sv_comments = {}\n",
    "for pair in valid_pairs:\n",
    "    site_a, site_b = pair.split('-')\n",
    "\n",
    "    # Drop NaN rows to get only valid shared tiepoints\n",
    "    sv_synchros[pair] = sv_links[[site_a, site_b]].dropna()\n",
    "    sv_synchros[pair].columns = ['depth1', 'depth2']\n",
    "    sv_synchros[pair]['age_unc'] = 5\n",
    "    sv_synchros[pair]['comment'] = np.nan  # Initialize age column with NaN\n",
    "\n",
    "    sv_comments[pair] = comment_add\n",
    "\n",
    "#now load existing synchros\n",
    "os.chdir('/Users/quinnmackay/Documents/GitHub/BICC/Paleochrono BICC Work/Paleochrono BICC Experiment/BICC2025')\n",
    "existing_synchros = {}\n",
    "existing_comments = {}\n",
    "\n",
    "for pair in valid_pairs:\n",
    "    folder_path = pair\n",
    "    file_path = os.path.join(folder_path, \"iceice_synchro_horizons.txt\")\n",
    "\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "\n",
    "        comments = []\n",
    "        with open(file_path, 'r') as f: # store existing comments to be re-added\n",
    "            for line in f:\n",
    "                if line.startswith('#'):\n",
    "                    comments.append(line.rstrip('\\n'))  # keeps tabs intact  # store comment lines without newline\n",
    "        existing_comments[pair] = comments\n",
    "\n",
    "        synchros = pd.read_csv(file_path, sep='\\t', comment='#')\n",
    "        existing_synchros[pair] = synchros\n",
    "        print(f\"Loaded {pair}: {len(synchros)} rows\")\n",
    "    else:\n",
    "        print(f\"Skipped {pair} â€” file not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Partial overlap detected for pair GRIP-GISP2 at new tiepoint [455.32385 481.03342   5.            nan]. One column is within 0.1 of an existing tiepoint, while other is not. The tiepoint was not added to avoid conflicts.\n",
      "Warning: Partial overlap detected for pair GRIP-GISP2 at new tiepoint [931.799   978.10284   5.            nan]. One column is within 0.1 of an existing tiepoint, while other is not. The tiepoint was not added to avoid conflicts.\n",
      "Warning: Partial overlap detected for pair GRIP-GISP2 at new tiepoint [1126.6066 1178.7477    5.           nan]. One column is within 0.1 of an existing tiepoint, while other is not. The tiepoint was not added to avoid conflicts.\n",
      "Warning: Partial overlap detected for pair GRIP-GISP2 at new tiepoint [1269.4423 1326.0201    5.           nan]. One column is within 0.1 of an existing tiepoint, while other is not. The tiepoint was not added to avoid conflicts.\n",
      "Warning: Partial overlap detected for pair GRIP-GISP2 at new tiepoint [1354.7086 1413.7722    5.           nan]. One column is within 0.1 of an existing tiepoint, while other is not. The tiepoint was not added to avoid conflicts.\n",
      "Saved merged synchro for GRIP-GISP2 with rows to GRIP-GISP2/iceice_synchro_horizons.txt\n",
      "There were 822 existing rows, 8 overlaps (5 error overlaps), and 0 new ties.\n"
     ]
    }
   ],
   "source": [
    "tolerance = 0.10  # meters\n",
    "\n",
    "for pair in valid_pairs:\n",
    "\n",
    "    if pair in existing_synchros:\n",
    "        \n",
    "        overlaps = 0\n",
    "        new_ties = 0\n",
    "        error_overlaps = 0\n",
    "\n",
    "        existing = existing_synchros[pair].copy(deep=True)\n",
    "        new = sv_synchros[pair].copy(deep=True)\n",
    "\n",
    "        total_existing = len(existing)\n",
    "\n",
    "        # Start with originals (priority)\n",
    "        combined = existing.copy(deep=True)\n",
    "\n",
    "        # Append only existing tiepoints that are NOT close to any new tiepoint\n",
    "        for idx, row in new.iterrows():\n",
    "\n",
    "            mask_either = ( #check for either column within tolerance. Does NOT append if either is close.\n",
    "                (abs(existing.iloc[:, 0] - row.iloc[0]) <= tolerance) |\n",
    "                (abs(existing.iloc[:, 1] - row.iloc[1]) <= tolerance)\n",
    "            )\n",
    "            mask_both = ( #check for both columns to spit out error if needed\n",
    "                (abs(existing.iloc[:, 0] - row.iloc[0]) <= tolerance) &\n",
    "                (abs(existing.iloc[:, 1] - row.iloc[1]) <= tolerance)\n",
    "            )\n",
    "\n",
    "            if not mask_either.any():\n",
    "                combined = pd.concat([combined, row.to_frame().T], ignore_index=True)\n",
    "                new_ties+=1\n",
    "            else:\n",
    "                overlaps += 1\n",
    "\n",
    "            if mask_either.any() and not mask_both.any():\n",
    "                print(f\"Warning: Partial overlap detected for pair {pair} at new tiepoint {row.values}. One column is within {tolerance} of an existing tiepoint, while other is not. The tiepoint was not added to avoid conflicts.\")\n",
    "                error_overlaps += 1\n",
    "\n",
    "        #duplicate check\n",
    "        before = len(combined)\n",
    "        combined = combined.drop_duplicates(ignore_index=True)\n",
    "        after = len(combined)\n",
    "\n",
    "        if after < before:\n",
    "            print(f\"Warning: {before - after} duplicate tiepoint(s) found and removed in pair {pair}.\")\n",
    "\n",
    "        folder_path = pair\n",
    "        file_path = os.path.join(folder_path, \"iceice_synchro_horizons.txt\")\n",
    "        os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "        # --- Gather comments ---\n",
    "        new_comments = sv_comments.get(pair, [])\n",
    "\n",
    "        with open(file_path, 'w') as f:\n",
    "            # Write new comments first\n",
    "            for line in new_comments:\n",
    "                f.write(f\"{line}\\n\")\n",
    "            # Add original comments\n",
    "            for line in existing_comments.get(pair, []):\n",
    "                f.write(f\"{line}\\n\")\n",
    "            # Write merged DataFrame below\n",
    "            combined.to_csv(f, sep='\\t', index=False)\n",
    "        print(f\"Saved merged synchro for {pair} with rows to {file_path}\\nThere were {total_existing} existing rows, {overlaps} overlaps ({error_overlaps} error overlaps), and {new_ties} new ties.\")\n",
    "\n",
    "    else:\n",
    "        folder_path = pair\n",
    "        file_path = os.path.join(folder_path, \"iceice_synchro_horizons.txt\")\n",
    "        os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "        combined = sv_synchros[pair].copy(deep=True) # svensson links load\n",
    "        new_comments = sv_comments.get(pair, [])\n",
    "\n",
    "        with open(file_path, 'w') as f:\n",
    "            # Write new comments first\n",
    "            for line in new_comments:\n",
    "                f.write(f\"{line}\\n\")\n",
    "            # Write merged DataFrame below\n",
    "            combined.to_csv(f, sep='\\t', index=False)\n",
    "\n",
    "        print(f\"No existing synchros for {pair}, saving new synchro to {file_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bicc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
